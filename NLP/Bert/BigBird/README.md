# Big Bird: Transformers for Longer Sequences 

# Big Bird: 更长序列的Transformer

## 摘要



基于 Transformers 的模型，例如 BERT，一直是 NLP 最成功的深度学习模型之一。 不幸的是，它们的核心限制之一是由于其完整的注意力机制，对序列长度的二次依赖（主要是在记忆方面）。 为了解决这个问题，我们提出了 BIGBIRD，这是一种稀疏注意机制，可以将这种二次依赖减少为线性。 我们证明了 BIGBIRD 是序列函数的通用逼近器并且是图灵完备的，从而保留了二次全注意模型的这些属性。在此过程中，我们的理论分析揭示了拥有 O(1) 全局令牌（例如 CLS）的一些好处，这些令牌将整个序列作为稀疏注意机制的一部分。 提议的稀疏注意力可以处理长度高达以前使用类似硬件可能达到的序列的 8 倍的序列。 由于能够处理更长的上下文，BIGBIRD 极大地提高了各种 NLP 任务（例如问答和摘要）的性能。 我们还提出了基因组学数据的新应用。

## 1 前言

基于 Transformers [91] 的模型，例如 BERT [22, 63]，在各种自然语言处理 (NLP) 任务中都非常成功，因此是现代 NLP 研究的支柱。 它们的多功能性和稳健性是 Transformer 被广泛采用的主要驱动力。 该模型很容易适应各种基于序列的任务——作为用于翻译 [91]、摘要 [66]、生成 [15] 等的 seq2seq 模型，或作为情感分析 [83]、词性标注的独立编码器 [65]、机器阅读理解 [93] 等——众所周知，它的性能大大优于以前的序列模型，如 LSTM [37]。  Transformers 的关键创新是引入了自注意力机制，可以对输入序列的每个标记进行并行评估，消除了循环神经网络（如 LSTM）中的顺序依赖性。 这种并行性使 Transformers 能够充分利用现代 SIMD 硬件加速器（如 GPU/TPU）的全部功能，从而促进在前所未有大小的数据集上训练 NLP 模型。 这种在大规模数据上进行训练的能力导致了 BERT [22] 和 T5 [75] 等模型的出现，这些模型在大型通用语料库上预训练 Transformer，并将知识转移到下游任务中。 预训练导致低数据机制下游任务 [51] 以及具有足够数据的任务 [101] 的显着改进，因此成为当代 NLP 中转换器无处不在的主要力量。

自注意力机制通过允许输入序列中的每个标记独立地关注序列中的每个其他标记，克服了 RNN 的约束（即 RNN 的顺序性质）。 这种设计选择有几个有趣的影响。 特别是，完全自注意力具有计算和内存要求，在序列长度上是二次的。 我们注意到，虽然语料库可能很大，但在许多应用程序中提供上下文的序列长度非常有限。 使用常用的当前硬件和模型大小，这个要求转化为大致能够处理长度为 512 个令牌的输入序列。 这降低了它对需要更大上下文的任务的直接适用性，如 QA [60]、**文档分类**等。

然而，虽然我们知道 self-attention 和 Transformers 很有用，但我们的理论理解还很初级。 自注意力模型的哪些方面对其性能是必要的？ 关于 Transformer 和类似模型的表现力，我们能说些什么？  Apriori，从设计中甚至不清楚所提出的自注意力机制是否与 RNN 一样有效。 例如，self-attention 甚至不遵守序列顺序，因为它是置换等变的。 正如 Yun 等人所言，这一问题已得到部分解决。  [104] 表明转换器具有足够的表现力，可以捕获所有连续序列到具有紧凑域的序列函数。 与此同时，佩雷斯等人。  [72]表明完整的变压器是图灵完备的（即可以模拟完整的图灵机）。 两个自然的问题出现了：我们能否使用更少的内积实现完全二次自注意力机制的经验收益？ 这些稀疏的注意力机制是否保留了原始网络的表现力和灵活性？

在本文中，我们解决了上述两个问题，并产生了一种**稀疏注意力机制**，以提高需要长上下文的多种任务的性能。 我们系统地开发了 BIGBIRD，这是一种注意力机制，其复杂性与令牌数量呈线性关系（第 2 节）。 我们从图稀疏化方法中汲取灵感，并了解当放松全注意力以形成建议的注意力模式时，Transformer 表达能力的证明在哪里失效。这种理解帮助我们开发了 BIGBIRD，它在理论上具有表现力，而且在经验上也很有用。 特别是，我们的 BIGBIRD 由三个主要部分组成：

- 一组 g 全局tokens参与序列的所有部分。
- 参与一组 w 个本地相邻tokens的所有tokens。
- 参与一组 r 个随机tokens的所有tokens。

这导致高性能的注意力机制可扩展到更长的序列长度（8x）。

总而言之，我们的主要贡献是：

1. BIGBIRD 满足全transformer的所有已知理论特性（第 3 节）。 特别是，我们表明添加额外的标记允许将所有连续序列表达为仅具有 O(n) 内积的序列函数。 此外，我们表明，在关于精度的标准假设下，BIGBIRD 是图灵完备的。
2. 根据经验，我们表明由 BIGBIRD 建模的扩展上下文有益于各种 NLP 任务。 我们在许多不同的数据集上实现了最先进的问答和文档摘要结果。 这些结果的摘要在第 4节中介绍。 
3. 最后，我们介绍了一种基于注意力模型的新应用，其中长上下文是有益的：提取 DNA 等基因组序列的上下文表示。 通过更长的掩蔽 LM 预训练，BIGBIRD 提高了下游任务的性能，例如启动子区域和染色质轮廓预测（第 5 节）。

### 1.1 相关工作

已经有许多有趣的尝试，旨在减轻 Transformer 的二次依赖，这可以大致分为两个方向。 第一线工作包含长度限制并围绕它开发方法。 此类别中最简单的方法仅使用滑动窗口 [93]，但通常大多数工作都符合以下一般范式：使用其他机制选择相关上下文的较小子集来馈入转换器并可选地进行迭代，即调用转换器块多个 每次都有不同的上下文。最突出的是，SpanBERT [42]、ORQA [54]、REALM [34]、RAG [57] 在不同任务上都取得了强大的性能。 然而，值得注意的是，这些方法通常需要大量的工程工作（例如通过大规模最近邻搜索进行反向传播）并且难以训练。

第二线工作问题是全注意力机制是否必不可少，并尝试提出不需要全注意力机制的方法，从而减少内存和计算要求。值得注意的是，戴等人。  [21]，Sukhbaatar 等人。  [82]，雷等人。  [74] 提出了自回归模型，该模型适用于从左到右的语言建模，但在需要双向上下文的任务中会受到影响。 Child等人 [16] 提出了一种将复杂度降低到 O(n√n) 的稀疏模型，Kitaev 等人。  [49] 通过使用 LSH 计算最近邻，进一步将复杂性降低到 O(n log(n))。

![]()

图 1：BIGBIRD 中使用的注意力机制的构建块。 白色表示注意力不集中。  (a) r = 2 的随机注意力，(b) w = 3 的滑动窗口注意力 (c) g = 2 的全局注意力。 (d) 组合 BIGBIRD 模型。

叶等人。  [103] 提出了数据的二进制分区，其中 Qiu 等人。  [73] 通过使用块稀疏性降低了复杂性。 最近，Longformer [8] 引入了一种基于局部滑动窗口的掩码，其全局掩码很少，以减少计算并将 BERT 扩展到更长的基于序列的任务。 最后，我们的工作与 Extended Transformers Construction [4] 的工作密切相关并建立在其之上。这项工作旨在为转换器在文本中编码结构。 他们广泛使用全球代币的想法来实现他们的目标。 我们的理论工作也可以被视为为这些模型的成功提供了理由。 需要注意的是，上述大多数方法都是基于启发式的，并且根据经验并不像原始转换器那样通用和稳健，即相同的架构在多个标准基准上无法获得 SoTA。（Longformer 有一个例外，我们将其包含在我们所有的比较中，有关更详细的比较，请参见 App. E.3）。 此外，这些近似值没有理论上的保证。

## 2 BigBird 结构

在本节中，我们使用广义注意机制来描述 BIGBIRD 模型，该机制用于在输入序列 X = (x1, ..., xn) ∈ Rn×d 上运行的每个transformer层。广义注意力机制由一个有向图 D 描述，其顶点集为 [n] = {1, .  .  .  ，n}。 弧集（有向边）表示注意力机制将考虑的内积集。 设 N(i) 表示 D 中节点 i 的外邻居集，则广义注意力机制的第 i 个输出向量定义为

![]()











