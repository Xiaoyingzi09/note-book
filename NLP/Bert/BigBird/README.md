# Big Bird: Transformers for Longer Sequences 

# Big Bird: 更长序列的Transformer

## 摘要



基于 Transformers 的模型，例如 BERT，一直是 NLP 最成功的深度学习模型之一。 不幸的是，它们的核心限制之一是由于其完整的注意力机制，对序列长度的二次依赖（主要是在记忆方面）。 为了解决这个问题，我们提出了 BIGBIRD，这是一种稀疏注意机制，可以将这种二次依赖减少为线性。 我们证明了 BIGBIRD 是序列函数的通用逼近器并且是图灵完备的，从而保留了二次全注意模型的这些属性。在此过程中，我们的理论分析揭示了拥有 O(1) 全局token（例如 CLS）的一些好处，这些token将整个序列作为稀疏注意机制的一部分。 提议的稀疏注意力可以处理长度高达以前使用类似硬件可能达到的序列的 8 倍的序列。 由于能够处理更长的上下文，BIGBIRD 极大地提高了各种 NLP 任务（例如问答和摘要）的性能。 我们还提出了基因组学数据的新应用。

## 1 前言

基于 Transformers [91] 的模型，例如 BERT [22, 63]，在各种自然语言处理 (NLP) 任务中都非常成功，因此是现代 NLP 研究的支柱。 它们的多功能性和稳健性是 Transformer 被广泛采用的主要驱动力。 该模型很容易适应各种基于序列的任务——作为用于翻译 [91]、摘要 [66]、生成 [15] 等的 **seq2seq 模型**，或作为情感分析 [83]、词性标注的独立编码器 [65]、机器阅读理解 [93] 等——众所周知，它的性能大大优于以前的序列模型，如 LSTM [37]。  Transformers 的关键创新是引入了自注意力机制，可以对输入序列的每个标记进行并行评估，消除了循环神经网络（如 LSTM）中的顺序依赖性。 这种并行性使 Transformers 能够充分利用现代 SIMD 硬件加速器（如 GPU/TPU）的全部功能，从而促进在前所未有大小的数据集上训练 NLP 模型。 这种在大规模数据上进行训练的能力导致了 BERT [22] 和 T5 [75] 等模型的出现，这些模型在大型通用语料库上预训练 Transformer，并将知识转移到下游任务中。 预训练导致低数据机制下游任务 [51] 以及具有足够数据的任务 [101] 的显着改进，因此成为当代 NLP 中转换器无处不在的主要力量。

自注意力机制通过允许输入序列中的每个标记独立地关注序列中的每个其他标记，克服了 RNN 的约束（即 RNN 的顺序性质）。 这种设计选择有几个有趣的影响。 特别是，完全自注意力具有计算和内存要求，在序列长度上是二次的。 我们注意到，虽然语料库可能很大，但在许多应用程序中提供上下文的序列长度非常有限。 使用常用的当前硬件和模型大小，这个要求转化为大致能够处理长度为 512 个token的输入序列。 这降低了它对需要更大上下文的任务的直接适用性，如 QA [60]、**文档分类**等。

然而，虽然我们知道 self-attention 和 Transformers 很有用，但我们的理论理解还很初级。 自注意力模型的哪些方面对其性能是必要的？ 关于 Transformer 和类似模型的表现力，我们能说些什么？  Apriori，从设计中甚至不清楚所提出的自注意力机制是否与 RNN 一样有效。 例如，self-attention 甚至不遵守序列顺序，因为它是置换等变的。 正如 Yun 等人所言，这一问题已得到部分解决。  [104] 表明转换器具有足够的表现力，可以捕获所有连续序列到具有紧凑域的序列函数。 与此同时，佩雷斯等人。  [72]表明完整的transformer是图灵完备的（即可以模拟完整的图灵机）。 两个自然的问题出现了：我们能否使用更少的内积实现完全二次自注意力机制的经验收益？ 这些稀疏的注意力机制是否保留了原始网络的表现力和灵活性？

在本文中，我们解决了上述两个问题，并产生了一种**稀疏注意力机制**，以提高需要长上下文的多种任务的性能。 我们系统地开发了 BIGBIRD，这是一种注意力机制，其复杂性与token数量呈线性关系（第 2 节）。 我们从图稀疏化方法中汲取灵感，并了解当放松全注意力以形成建议的注意力模式时，Transformer 表达能力的证明在哪里失效。这种理解帮助我们开发了 BIGBIRD，它在理论上具有表现力，而且在经验上也很有用。 特别是，我们的 BIGBIRD 由三个主要部分组成：

- 一组 g 全局tokens参与序列的所有部分。
- 参与一组 w 个本地相邻tokens的所有tokens。
- 参与一组 r 个随机tokens的所有tokens。

这导致高性能的注意力机制可扩展到更长的序列长度（8x）。

总而言之，我们的主要贡献是：

1. BIGBIRD 满足全transformer的所有已知理论特性（第 3 节）。 特别是，我们表明添加额外的标记允许将所有连续序列表达为仅具有 O(n) 内积的序列函数。 此外，我们表明，在关于精度的标准假设下，BIGBIRD 是图灵完备的。
2. 根据经验，我们表明由 BIGBIRD 建模的扩展上下文有益于各种 NLP 任务。 我们在许多不同的数据集上实现了最先进的问答和文档摘要结果。 这些结果的摘要在第 4节中介绍。 
3. 最后，我们介绍了一种基于注意力模型的新应用，其中长上下文是有益的：提取 DNA 等基因组序列的上下文表示。 通过更长的掩蔽 LM 预训练，BIGBIRD 提高了下游任务的性能，例如启动子区域和染色质轮廓预测（第 5 节）。

### 1.1 相关工作

已经有许多有趣的尝试，旨在减轻 Transformer 的二次依赖，这可以大致分为两个方向。 第一线工作包含长度限制并围绕它开发方法。 此类别中最简单的方法仅使用滑动窗口 [93]，但通常大多数工作都符合以下一般范式：使用其他机制选择相关上下文的较小子集来馈入转换器并可选地进行迭代，即调用转换器块多个 每次都有不同的上下文。最突出的是，SpanBERT [42]、ORQA [54]、REALM [34]、RAG [57] 在不同任务上都取得了强大的性能。 然而，值得注意的是，这些方法通常需要大量的工程工作（例如通过大规模最近邻搜索进行反向传播）并且难以训练。

第二线工作问题是全注意力机制是否必不可少，并尝试提出不需要全注意力机制的方法，从而减少内存和计算要求。值得注意的是，戴等人。  [21]，Sukhbaatar 等人。  [82]，雷等人。  [74] 提出了自回归模型，该模型适用于从左到右的语言建模，但在需要双向上下文的任务中会受到影响。 Child等人 [16] 提出了一种将复杂度降低到 O(n√n) 的稀疏模型，Kitaev 等人。  [49] 通过使用 LSH 计算最近邻，进一步将复杂性降低到 O(n log(n))。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/Bert/BigBird/figure/figure_1.png)

图 1：BIGBIRD 中使用的注意力机制的构建块。 白色表示注意力不集中。  (a) r = 2 的随机注意力，(b) w = 3 的滑动窗口注意力 (c) g = 2 的全局注意力。 (d) 组合 BIGBIRD 模型。

叶等人。  [103] 提出了数据的二进制分区，其中 Qiu 等人。  [73] 通过使用块稀疏性降低了复杂性。 最近，Longformer [8] 引入了一种基于局部滑动窗口的掩码，其全局掩码很少，以减少计算并将 BERT 扩展到更长的基于序列的任务。 最后，我们的工作与 Extended Transformers Construction [4] 的工作密切相关并建立在其之上。这项工作旨在为transformer在文本中编码结构。 他们广泛使用全局token的想法来实现他们的目标。 我们的理论工作也可以被视为为这些模型的成功提供了理由。 需要注意的是，上述大多数方法都是基于启发式的，并且根据经验并不像原始transformer那样通用和稳健，即相同的架构在多个标准基准上无法获得 SoTA。（Longformer 有一个例外，我们将其包含在我们所有的比较中，有关更详细的比较，请参见 App. E.3）。 此外，这些近似值没有理论上的保证。

## 2 BigBird 结构

在本节中，我们使用广义注意机制来描述 BIGBIRD 模型，该机制用于在输入序列 X = (x1, ..., xn) ∈ Rn×d 上运行的每个transformer层。广义注意力机制由一个有向图 D 描述，其顶点集为 [n] = {1, .  .  .  ，n}。 弧集（有向边）表示注意力机制将考虑的内积集。 设 N(i) 表示 D 中节点 i 的外邻居集，则广义注意力机制的第 i 个输出向量定义为

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/Bert/BigBird/figure/formula_1.png)

其中 Qh, Kh : Rd → Rm 分别是查询和关键函数，Vh : Rd → Rd 是一个值函数，σ 是一个评分函数（例如 softmax 或 hardmax），H 表示正面的数量。 还要注意 XN(i) 对应于仅通过堆叠 {xj : j ∈ N(i)} 而不是所有输入形成的矩阵。如果 D 是完整的有向图，我们将恢复 Vaswani 等人的完整二次注意力机制。[91]。 为了简化我们的说明，我们将对图 D 的邻接矩阵 A 进行操作，即使底层图可能是稀疏的。 详细说明，如果查询 i 关注键 j，则 A ∈ [0, 1]n×n 且 A(i, j) = 1，否则为零。 例如，当 A 是一个矩阵时（如在 BERT 中），它会导致二次复杂度，因为所有token都参与其他token。 这种将自注意力视为全连接图的观点使我们能够利用现有的图论来帮助降低其复杂性。降低 self-attention 的二次复杂度的问题现在可以看作是一个图稀疏化问题。 众所周知，随机图是扩展器，可以在许多不同的上下文中近似完整的图，包括它们的谱属性 [80, 38]。 我们认为注意力机制的稀疏随机图应该有两个要求：节点之间的小平均路径长度和局部性的概念，我们将在下面讨论每一个。

让我们考虑最简单的随机图构造，称为 Erd˝os-Rényi 模型，其中每条边都以固定概率独立选择。 在这样一个只有 ˜Θ(n) 边的随机图中，任意两个节点之间的最短路径是节点数量的对数 [17, 43]。 因此，这样的随机图在谱上近似于完整图，并且其第二个特征值（邻接矩阵的）与第一个特征值 [9, 10, 6] 相距甚远。 此属性导致 grpah 中随机游走的快速混合时间，这非正式地表明信息可以在任何一对节点之间快速流动。 因此，我们提出了一种稀疏注意，其中每个查询都涉及 r 个随机数量的密钥，即对于 r 个随机选择的密钥，A(i, ·) = 1（见图 1a）。

激发 BIGBIRD 创建的第二个观点是 NLP 和计算生物学中的大多数上下文都有显示大量参考位置的数据。 在这种现象中，一个token的大量信息可以从它的相邻token中推导出来。最相关的是，克拉克等人。  [19] 研究了 NLP 任务中的自注意力模型，并得出结论，相邻的内积非常重要。 局部性的概念，语言结构中标记的邻近性，也构成了各种语言理论的基础，如转换生成语法。 在图论的术语中，聚类系数是连通性局部性的度量，当图包含许多团或近团（几乎完全互连的子图）时，聚类系数很高。 简单的 Erd˝os-Rényi 随机图没有高聚类系数 [84]，但一类随机图，称为小世界图，表现出高聚类系数 [94]。  Watts 和 Strogatz [94] 引入的一个特定模型与我们高度相关，因为它在平均最短路径和局部性概念之间实现了良好的平衡。 他们的模型的生成过程如下：构建一个规则的环格，一个有 n 个节点的图，每个节点连接到 w 个邻居，每边 w/2。

换句话说，我们从节点上的滑动窗口开始。 然后将所有连接的随机子集 (k%) 替换为随机连接。保留其他 (100 - k)% 的本地连接。然而，删除这样的随机边在现代硬件上可能效率低下，所以我们保留它，这不会影响它的属性。 总之，为了在上下文中捕获这些局部结构，在 BIGBIRD 中，我们定义了一个滑动窗口注意力，以便在宽度为 w 的自注意力期间，在位置 i 处的查询从 i − w/2 到 i + w/2 个键参与。 在我们的符号中，A(i, i−w/2 : i+w/2) = 1（见图 1b）。 作为最初的健全性检查，我们进行了基本实验，以测试这些直觉是否足以使性能接近 BERT 之类的模型，同时保持注意力与token数量的线性关系。 我们发现随机块和局部窗口不足以捕获与 BERT 的性能竞争所需的所有上下文。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/Bert/BigBird/figure/table_1.png)

表 1：构建块比较@512

BIGBIRD 的最后一部分灵感来自我们的理论分析（第 3 节），这对经验表现至关重要。 更具体地说，我们的理论利用了“全局标记”（关注序列中所有标记的标记以及所有标记都参与的标记（见图 1c）的重要性）。这些全局标记可以通过两种方式定义：

- BIGBIRD-ITC：在内部transformer构造 (ITC) 中，我们将一些现有的代币设为“全局”，它们参与整个序列。 具体来说，我们选择索引的子集 G（g := |G|），使得 A(i, :) = 1 且 A(:, i) = 1 对于所有 i ∈ G。
- BIGBIRD-ETC：在扩展transformer构造 (ETC) 中，我们包含了额外的“全局”token，例如 CLS。 具体来说，我们添加 g 个全局token来处理所有现有token。 在我们的记法中，这对应于通过向矩阵 A 添加 g 行来创建一个新矩阵 B ∈ [0, 1](N+g)×(N+g)，使得 B(i, :) = 1，并且 B  (:, i) = 1 对于所有 i ∈ {1, 2, .  .  .  g}, 并且 B(g + i, g + j) = A(i, j)∀ i, j ∈ {1, .  .  .  ，N}。 这增加了额外的位置来存储上下文，正如我们将在实验中看到的那样提高了性能。

BIGBIRD 的最终注意力机制（图 1d）具有所有三个属性：查询关注 r 个随机键，每个查询关注其位置左侧的 w/2 个token和其位置右侧的 w/2 个token，以及 它们包含 g 个全局token（全局token可以来自现有token或额外添加的token）。 我们在 App  D.中提供了实现细节。 

## 3  稀疏注意力机制的理论结果

在本节中，我们将展示稀疏注意力机制在两个方面与全注意力机制一样强大和富有表现力。 首先，我们展示了在独立编码器（例如 BERT）中使用稀疏注意机制时，它们是 Yun 等人风格的序列到序列函数的通用逼近器。  [104]。 我们注意到，这一特性也在当代工作 Yun 等人的理论上进行了探索。  [105]。 其次，与 [105] 不同，我们进一步表明稀疏编码器-解码器转换器是图灵完备的（假设在 [72] 中定义的条件相同）。 作为上述积极结果的补充，我们还表明，转向稀疏注意力机制会产生成本，即没有免费的午餐。 在 3.4 小结中，我们通过展示任何足够稀疏的机制都需要多项式更多层的自然任务来显示下界。

### 3.1 符号

完整的 Transformer 编码器堆栈只不过是单层编码器（具有独立参数）的重复应用。 我们用 TH,m,q D 表示这种 Transformer 编码器堆栈的类别，使用广义编码器（第 2 节）定义，它由头部大小为 m 的 H 头组成，q 是输出网络的隐藏层大小，并且 注意层由有向图 D 定义。

我们提出的注意力机制与 Vaswani 等人的注意力机制之间的主要区别。  [91]，云等人。[104] 是我们在每个序列的开头添加一个特殊的标记，并为其分配一个特殊的向量。我们将其称为 x0。 因此，我们的图 D 将具有顶点集 {0} ∪ [n] = {0, 1, 2, ...  .  .  ，n}。我们将假设这个额外的节点及其各自的向量将被丢弃在 Transformer 的最终输出层。 为了避免繁琐的符号，我们仍将变换器视为映射序列 X ∈ Rn×d 到 Rn×d。 我们还将允许转换器将位置嵌入 E ∈ Rd×n 附加到输入层中的矩阵 X。

最后，我们需要定义函数类和距离测度来证明通用逼近性质。 让 FCD 表示连续函数 f 的集合：[0, 1]n×d → Rn×d，它们对于由 ℓp 范数定义的拓扑是连续的。 回想一下任何 p ≥ 1，ℓp 距离是 dp(f1, f2) = �� ∥f1(X) − f2(X)∥p pdX �1/p。

### 3.2 通用逼近器

定义 1. 以 0 为中心的星形图 S 是定义在 {0, .  .  .  ，n}。 所有顶点 i 的邻域为 N(i) = {0, i}，因为 i ∈ {1 。  .  .  n} 和 N(0) = {1, .  .  .  n}

我们的主要定理是任何包含 S 的图定义的稀疏注意机制是一个通用逼近器：

定理 1. 给定 1 < p < ∞ 和 ϵ > 0，对于任何 f ∈ FCD，存在一个具有稀疏注意力的变换器，g ∈ TH,m,q D 使得 dp(f, g) ≤ ϵ 其中 D 是 任何包含星形图 S 的图。为了证明定理，我们将遵循 [104] 中概述的标准证明结构。

步骤 1：通过分段常数函数近似 FCD。 由于 f 是一个有界域 [0, 1)n×d 的连续函数，我们将用一个合适的分段常数函数来近似它。这是通过将区域 [0, 1) 适当划分为粒度为 δ 的网格来实现的，以获得离散集 Gδ。 因此，我们可以假设我们正在处理一个函数 ¯f ：Gδ → Rn×d，其中 dp(f, ¯f) ≤ ϵ 3。

第 2 步：通过修改后的transformer近似分段常数函数。 这是证明的关键步骤，其中使用自注意力机制生成输入的上下文映射。 非正式地，上下文映射是由矩阵 (X, xi) 和列组成的对的唯一代码。 它的唯一性允许前馈层使用每个代码将其映射到唯一的输出列。

主要的技术挑战是仅使用稀疏注意机制计算上下文映射。 这是在 [104] 中使用“选择性”移位运算符完成的，该运算符将特定间隔中的条目向上移位。 他们证明的关键是这样一个事实，即这种转变正是最大条目到最小条目的范围。

创建具有稀疏注意力机制的上下文映射是一个相当大的挑战。 特别是，因为每个查询只关注几个键，所以完全不清楚是否可以收集足够的信息来制作整个矩阵的上下文嵌入。 为了解决这个问题，我们开发了一个稀疏移位算子，如果它们位于某个范围内，它会移动矩阵的条目。 确切的偏移量由有向稀疏注意力图 D 控制。第二个关键因素是使用额外的全局标记。 通过仔细地将运算符应用于一组选定的范围，我们将展示每列将包含完整映射的唯一映射。 因此，我们可以通过使用多层和辅助全局token来增加自我注意机制中内积的损失。

第 3 步：通过原始 Transformers 近似修改后的 Transformers：最后一步是通过使用 ReLU 和 softmax 的原始 Transformer 近似修改后的 Transformer。我们在应用程序A中提供完整的详细信息。

### 3.3 图灵完备

Transformers是一个非常通用的类。 在 Vaswani 等人的原始论文中。  [91]，它们被用于编码器和解码器。 虽然上一节概述了编码器的强大功能，但另一个自然的问题是询问解码器和编码器的额外功率是多少？ 佩雷斯等人。  [72]表明基于二次注意力机制的完整转换器是图灵完备的。 这一结果做出了一个不切实际的假设，即该模型适用于任意精度模型。 当然，这是必要的，否则，Transformer 是有界有限状态机，不能是图灵完备的。

很自然地会问是否需要完整的注意力机制。 或者也可以使用稀疏注意力机制来模拟任何图灵机？ 我们证明情况确实如此：我们可以使用稀疏编码器和稀疏解码器来模拟任何图灵机。

为了在 Transformer 架构中使用稀疏注意力机制，我们需要定义一个合适的修改，其中每个标记只对之前的标记做出反应。 与 BERT 的情况不同，整个注意力机制只应用一次，在完整的转换器中，解码器端的稀疏注意力机制是逐个token使用的。 其次是佩雷斯等人的工作。  [72]，使用每个标记作为磁带历史的表示，并使用全部注意力来移动和检索正确的磁带符号。 佩雷斯等人的大部分建设。  [72] 经历了稀疏的关注，除了它们指向历史的寻址方案（[72] 中的引理 B.4）。 我们展示了如何使用稀疏注意力机制来模拟这一点，并将细节推迟到 App B 中。

### 3.4 限制

我们展示了一个自然任务，可以通过 O(1) 层中的完整注意力机制来解决。然而，在标准复杂性理论假设下，对于任何具有 ~O(n) 边缘的稀疏注意力层（不仅仅是 BIGBIRD），这个问题需要 ~Ω(n) 层。  （这里 ~O 隐藏了多对数因子）。 考虑在给定的长度为 n 的序列中为每个向量找到对应的最远向量的简单问题。 正式地，

任务 1. 给定 n 个单位向量 {u1, .  .  .  , un}, 找到 f(u1, . . . , un) → (u1∗, . . . , un∗) 其中对于固定的 j ∈ [n]，我们定义 j∗ = arg maxk ∥uk − uj∥2  2.在单位向量的情况下，查找相距最远的向量归结为最小化内积搜索。 对于具有适当查询和键的全注意机制，此任务非常简单，因为我们可以评估所有成对的内积。

由于正交向量猜想 (OVC) [1, 2, 7, 96] 的硬度结果，稀疏注意力是不可能的。  OVC 是细粒度复杂性中广泛使用的假设。

非正式地，它指出无法确定在次二次时间内 n 个布尔向量之间的最小内积是否为 0。 在应用程序中。  C，我们展示了使用 OVC 的简化，以表明如果任何稀疏有向图 D 的变换器 g ∈ T H=1,m=2d,q=0 D 可以评估任务 1，则它可以解决正交向量问题。

命题 1. 存在单层完全自注意力 g ∈ T H=1,m=2d,q=0 可以评估任务 1，即 g(u1, ..., un) = [u1∗, ...  .  .  , un∗]，但对于任何具有 ~O(n) 边（即内积评估）的稀疏注意图 D，将需要 ~Ω(n1−o(1)) 层。我们在 App C中给出了这个事实的正式证明。

## 4 实验： 自然语言处理（NLP）

在本节中，我们的目标是展示为 NLP 任务建模更长输入序列的好处，为此我们选择了三个有代表性的任务。 我们从基本的掩码语言建模（MLM；Devlin 等人 22）开始，以检查是否可以通过使用更长的连续序列来学习更好的上下文表示。 接下来，我们考虑具有支持证据的 QA，为此，处理更长序列的能力将使我们能够使用 TF-IDF/BM25 等原始系统检索更多证据。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/Bert/BigBird/figure/table_2.png)

表 2：使用基本尺寸模型的 QA Dev 结果。 我们报告了 HotpotQA、Natural Questions 和 TriviaQA 的 WikiHop 和 F1 的准确性。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/Bert/BigBird/figure/table_3.png)

表 3：QA 任务测试集的微调结果。 测试结果（HotpotQA 的 F1、Natural Questions、TriviaQA 和 WikiHop 的准确度）是从各自的排行榜中挑选出来的。对于每项任务，选出前 3 名领导者，不包括 BIGBIRD 等。 对于自然问题长答案 (LA)、TriviaQA 和 WikiHop，BIGBIRD-ETC 是最新的技术。 在 HotpotQA 上，我们在 F1 排行榜上排名第三，在 Exact Match (EM) 上排名第二。

最后，我们处理长文档分类，其中区分信息可能不在前 512 个标记中。 下面我们总结了使用序列长度 40961 的 BIGBIRD 的结果，而我们将所有其他设置细节（包括计算资源、批量大小、步长）推迟到 App E.

**预训练和 MLM** 我们按照 [22, 63] 创建 BIGBIRD 的基本和大型版本，并使用 MLM 目标对其进行预训练。 此任务涉及预测已被屏蔽的随机token子集。 我们使用四个标准数据集进行预训练（在 App. E.1 的 Tab. 9 中列出），从公共 RoBERTa checkpoint2 热启动。 我们根据每个字符的位数比较了预测被屏蔽标记的性能，遵循 [8]。 正如在应用程序中看到的那样。  E.1，表。  10、BIGBIRD 和 Longformer 的表现都优于有限长度的 RoBERTa，其中 BIGBIRD-ETC 表现最好。 我们注意到我们在合理的 16GB 内存/芯片上训练我们的模型，批量大小为 32-64。 我们的记忆效率归因于第 2 节中描述的稀疏注意机制的有效阻塞和稀疏结构。 

**问答 (QA)** 我们考虑了以下四个具有挑战性的数据集：

1. 自然问题 [52]：对于给定的问题，从给定的证据中找到一小段答案 (SA)，并突出显示给定证据中包含正确答案 (LA) 信息的段落。
2. HotpotQA-distractor [100]：与自然问题类似，它需要从给定的证据中找到多跳推理所需的不同文档的答案（Ans）以及支持事实（Sup）。
3. TriviaQA-wiki [41]：我们需要使用提供的维基百科证据为给定的问题提供答案，但是，答案可能不存在于给定的证据中。 在经过验证的较小问题子集上，保证给定的证据包含答案。尽管如此，在这种情况下，我们也将答案建模为跨度选择问题。
4. WikiHop [95]：通过汇总证据中给出的多个文档中的信息，从多项选择题 (MCQ) 中选择正确的选项。

由于这些任务非常具有竞争力，因此针对每个数据集设计了多个高度工程化的系统，以确认各自的输出格式。 为了公平比较，我们不得不使用一些额外的正则化来训练 BIGBIRD，详细信息在 App.  E.2 连同精确的架构描述。 我们使用基本大小的模型进行实验，并为每个数据集在开发集上选择最佳配置（如表 2 所示）。 我们可以看到，具有扩展全球代币的 BIGBIRD-ETC 始终优于所有其他模型。 因此，我们选择这种配置来训练一个大型模型，用于对隐藏测试集进行评估。

在选项卡中。  3，我们将 BIGBIRD-ETC 模型与排行榜中不包括 BIGBIRD 的前 3 名条目进行比较。
  可以清楚地看到使用较长上下文的重要性，因为 Longformer 和 BIGBIRD 都优于具有较小上下文的模型。 此外，值得注意的是，BIGBIRD 提交的是单个模型，而 Natural Questions 的其他前 3 个条目是集合，这可能解释了精确答案短语选择的准确性略低的原因。

**分类**我们在不同长度和内容的数据集上进行实验，特别是各种文档分类和 GLUE 任务。 在 BERT 之后，我们在第一个 [CLS] token之上使用了一个具有交叉熵损失的层。 我们看到，当我们拥有更长的文档和更少的训练示例时，使用 BIGBIRD 的收益更显着。 例如，使用基本大小的模型，BIGBIRD 将 Arxiv 数据集的最新技术提高了约 5%。 在 Patents 数据集上，使用简单的 BERT/RoBERTa 有改进，但考虑到大量的训练数据，对 SoTA（不是基于 BERT 的）的改进并不显着。 请注意，对于更小的 IMDb 数据集，看不到这种性能提升。 除了实验设置细节外，我们还在 App 中展示了详细的结果。  E.4 表现出有竞争力的表现。

### 4.1 编码器-解码器任务

对于编码器-解码器设置，可以很容易地看到，由于完全自我注意，两者都受到二次复杂性的影响。 我们只在编码器端重点介绍 BIGBIRD 的稀疏注意力机制。 这是因为，在实际的生成应用中，与输入相比，输出序列的长度通常很小。 例如，对于文本摘要，我们在现实场景中看到（参见 App. E.5 Tab. 18），输出序列长度的中位数为 ∼ 200，而输入序列的中位数长度 > 3000。对于此类应用，效率更高 编码器使用稀疏注意力机制，解码器使用完全自注意力机制。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/Bert/BigBird/figure/table_4.png)

表 4：长文档的总结 ROUGE 分数。

**摘要** 文档摘要是一项为文本文档创建简短而准确的摘要的任务。 我们使用了三个长文档数据集来测试我们的模型细节，表中提到了这些细节。  18. 在本文中，我们专注于长文档的抽象摘要，其中使用更长的上下文编码器应该可以提高性能。 原因有两个：首先，显着内容可以均匀分布在长文档中，而不仅仅是前 512 个标记，这是 BigPatents 数据集 [78] 中的设计。 其次，较长的文档展示了更丰富的话语结构，摘要更加抽象，因此观察更多的上下文有帮助。正如最近 [76, 107] 所指出的，预训练有助于生成任务，我们从基于基本模型的通用 MLM 预训练开始，并利用 Pegasus 的最先进的总结特定预训练 [107  ] 在大型模型上。 在这些长文档数据集上训练 BIGBIRD 稀疏编码器和完整解码器的结果在表中给出。  4. 我们可以清楚地看到建模更长的上下文带来了显着的改进。 除了超参数，我们还在 App 中展示了更短但更广泛的数据集的结果。  E.5，这表明使用稀疏注意力也不会影响性能。

 # 5 实验：基因组学

最近在基因组学数据中使用深度学习的热潮 [86, 106, 13]，这提高了几个具有生物学意义的任务的性能，例如启动子位点预测 [71]、甲基化分析 [55]、预测功能 非编码变体的影响 [109] 等。这些方法消耗 DNA 序列片段作为输入，因此我们相信 BIGBIRD 更长的输入序列处理能力将是有益的，因为 DNA 中的许多功能影响是高度非局部的 [12]。 此外，从 NLP 中汲取灵感，我们通过 MLM 预训练，利用丰富的未标记数据（例如人类参考基因组、酵母菌基因组数据库）学习了强大的 DNA 片段上下文表示。 接下来，我们展示了我们的长输入 BIGBIRD 以及建议的预训练显着提高了两个下游任务的性能。  App F中提供了这两个任务的详细实验设置。

**预训练和 MLM** 正如在 Liang [58] 中所探讨的那样，我们建议先将 DNA 分割成标记，以进一步增加上下文长度（App. F，图 7），而不是对碱基对进行操作。 特别是，我们为大小为 32K 的 DNA 序列构建了一个字节对编码 [50] 表，每个标记平均代表 8.78 个碱基对。 我们使用 MLM 目标在人类参考基因组 (GRCh37)3 上学习这些标记的上下文表示。 然后，我们在 Tab 中的保留集上报告每个字符的位数 (BPC)。  5. 我们发现基于注意力的 DNA 上下文表示确实提高了 BPC，通过使用更长的上下文可以进一步改善 BPC。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/Bert/BigBird/figure/table_5.png)

表 5：MLM BPC

**启动子区域预测**启动子是通常位于基因上游的 DNA 区域，它是转录起始位点。已经提出了多种方法来识别给定 DNA 序列中的启动子区域 [99, 59, 11, 98, 71]，因为这是了解基因调控的重要第一步。 相应的机器学习任务是将给定的 DNA 片段分类为启动子或非启动子序列。 我们使用 Oubounyt 等人编译的数据集。  [71] 由真核启动子数据库 (EPDnew) [24] 4. 我们从上面微调了预训练的 BIGBIRD 模型，使用训练数据和测试数据集上的 F1 报告。 我们将我们的结果与 Tab 中先前报告的最佳方法进行比较。  6. 我们看到 BIGBIRD 实现了近乎完美的准确度，比之前报告的最佳准确度提高了 5%。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/Bert/BigBird/figure/table_6.png)

表 6：比较。

**染色质分布预测** DNA 的非编码区域不编码蛋白质。 大多数疾病和其他性状相关的单核苷酸多态性与非编码基因组变异相关 [109, 46]。因此，了解 DNA 非编码区的功能影响是一项非常重要的任务。 如 Zhou 和Troyanskaya [109] 所定义，该过程中的一个重要步骤是从非编码基因组序列预测大规模染色质分析。 为此，DeepSea [109] 编译了来自 DNA 元素百科全书 (ENCODE)5 和路线图表观基因组学项目 6 的 240 万个非编码变体的 919 染色质图谱。 相应的 ML 任务是预测，对于给定的 DNA 非编码区，这 919 个染色质特征，包括 690 个转录因子 (TF) 结合特征，用于 160 个不同 TF、125 个 DNase I 敏感性 (DHS) 特征和 104 个组蛋白标记 (HM) 配置文件。 我们共同学习了 919 个二元分类器，以从 DNA 片段的序列中预测这些功能影响。 在保留的染色体上，我们将 AUC 与表中的基线进行比较。  7 并看到我们在更难的任务 HM 上的性能显着提高，众所周知，该任务具有比其他任务更长的相关性 [27]。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/Bert/BigBird/figure/table_7.png)

# 6 总结

我们提出了 BIGBIRD：一种与token数量呈线性关系的**稀疏注意机制**。  BIGBIRD 满足许多理论结果：它是序列到序列函数的通用逼近器，也是图灵完备的。 理论上，我们使用额外全局token的力量来保留模型的表达能力。 我们通过表明转向稀疏注意力机制确实会产生成本来补充这些结果。 根据经验，BIGBIRD 在许多 NLP 任务（例如问答和长文档分类）上提供了最先进的性能。 我们进一步为 DNA 引入了基于注意力的上下文语言模型，并针对下游任务对其进行了微调，例如启动子区域预测和非编码变体的预测效果。



