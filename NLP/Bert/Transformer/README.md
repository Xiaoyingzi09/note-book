# Attention Is All You Need

## 摘要

主导序列转导模型基于复杂的递归或卷积神经网络，包括编码器和解码器。性能最好的模型还通过注意机制连接编码器和解码器。我们提出了一种新的简单的网络结构，变压器，完全基于注意机制，完全不需要重复和卷积。在两个机器翻译任务上的实验表明，这些模型具有更高的质量，同时具有更高的并行性，所需的训练时间显著减少。我们的模型在WMT 2014英德翻译任务中达到28.4 BLEU，比现有的最佳结果（包括合奏）提高了2 BLEU以上。在WMT 2014英法翻译任务中，我们的模型在8个GPU上进行3.5天的培训后，建立了新的单模型最先进的BLEU分数41.8，这是文献中最佳模型培训成本的一小部分。通过将该变换器成功地应用于具有大量和有限训练数据的英语选区分析，我们证明了该变换器可以很好地推广到其他任务。



## 1 前言

递归神经网络，特别是长-短期记忆[13]和选通递归[7]神经网络，已被确定为序列建模和转换问题（如语言建模和机器翻译[35,2,5]）的最新方法。此后，大量的努力继续推动循环语言模型和编解码器架构的边界[38,24,15]。

递归模型通常沿输入和输出序列的符号位置进行因子计算。将位置与计算时间中的步骤对齐，它们生成一系列隐藏状态ht，作为先前隐藏状态ht的函数−1和位置t的输入。这种固有的顺序性质排除了训练示例中的并行化，这在较长的序列长度下变得至关重要，因为内存约束限制了跨示例的批处理。最近的工作通过因子分解技巧[21]和条件计算[32]显著提高了计算效率，同时也提高了后者的模型性能。然而，顺序计算的基本限制仍然存在。

注意机制已经成为各种任务中强制性序列建模和转导模型的一个组成部分，允许对依赖性进行建模，而不考虑它们在输入或输出序列中的距离[2,19]。然而，在除少数情况外的所有情况下[27]，这种注意机制都与循环网络结合使用。

在这项工作中，我们提出了Transformer，这是一种避免重复的模型架构，而完全依赖于一种注意机制来绘制输入和输出之间的全局依赖关系。在八个P100 GPU上训练12小时后，该转换器允许显著更多的并行化，并可达到翻译质量的最新水平。

## 2 研究背景

减少顺序计算的目的也形成了扩展神经GPU〔16〕、ByteNet〔18〕和VusS2S〔9〕的基础，所有这些都使用卷积神经网络作为基本构建块，并行计算所有输入和输出位置的隐藏表示。在这些模型中，将来自两个任意输入或输出位置的信号关联起来所需的操作数量随着位置之间的距离而增加，CONVS2为线性，ByteNet为对数。这使得了解远距离位置之间的依赖关系变得更加困难[12]。在变压器中，这被减少到一个恒定的操作数，尽管由于平均注意加权位置而降低了有效分辨率，这一影响我们用第3.2节中描述的多头部注意抵消。

自我注意，有时称为内部注意，是一种注意机制，将单个序列的不同位置联系起来，以计算序列的表示。自我注意已成功应用于多种任务，包括阅读理解、抽象总结、文本蕴涵和学习任务无关的句子表征[4,27,28,22]。

端到端记忆网络基于重复注意机制，而不是顺序一致的重复，并且在简单的语言问答和语言建模任务中表现良好[34]。

然而，据我们所知，Transformer是第一个完全依靠自我注意来计算其输入和输出表示的转导模型，而不使用序列对齐RNN或卷积。在以下章节中，我们将描述变压器，激发自我关注，并讨论其相对于[17,18]和[9]等模型的优势。

## 3 模型结构

大多数竞争性神经序列转导模型都有编码-解码结构[5,2,35]。这里，编码器将符号表示（x1，…，xn）的输入序列映射到连续表示z=（z1，…，zn）的序列。给定z，解码器然后一次生成一个元素的符号输出序列（y1，…，ym）。在每一步中，模型都是自回归的[10]，在生成下一步时，使用先前生成的符号作为额外输入。

转换器遵循这一总体架构，使用编码器和解码器的堆叠自关注和逐点全连接层，分别如图1的左半部分和右半部分所示。

![]()

图1：Transformer-model架构。

### 3.1 编码器和解码器堆栈

编码器：编码器由N=6个相同层的堆栈组成。每层有两个子层。第一种是多头自我注意机制，第二种是简单的位置全连接前馈网络。我们在两个子层的每个层周围使用剩余连接[11]，然后使用层规范化[1]。也就是说，每个子层的输出是LayerNorm（x+子层（x）），其中子层（x）是由子层本身实现的功能。为了方便这些剩余连接，模型中的所有子层以及嵌入层都会生成尺寸为dmodel=512的输出。

解码器：解码器也由N=6个相同层的堆栈组成。除了每个编码器层中的两个子层之外，解码器还插入第三个子层，该子层对编码器堆栈的输出执行多头注意。与编码器类似，我们在每个子层周围使用剩余连接，然后进行层规范化。我们还修改了解码器堆栈中的自我注意子层，以防止位置涉及后续位置。这种掩蔽，结合输出嵌入偏移一个位置的事实，确保位置i的预测只能依赖于位置小于i的已知输出。

### 3.2 注意力机制

注意函数可以描述为将查询和一组键值对映射到输出，其中查询、键、值和输出都是向量。输出作为值的加权和计算，其中分配给每个值的权重由查询与相应键的兼容函数计算。

![]()

图2：（左）缩放点产品注意事项。（右）多头注意力由几个平行运行的注意力层组成。

#### 3.2.1 Scaled Dot-Product Attention

我们称我们的特别关注为“缩放点产品关注”（图2）。输入包括维度dk的查询和键以及维度dv的值。我们用所有键计算查询的点积，每个键除以√dk，并应用softmax函数以获得值的权重。

在实践中，我们同时计算一组查询上的注意函数，将它们打包成矩阵Q。键和值也打包成矩阵K和V。我们将输出矩阵计算为：

![]()

两个最常用的注意函数是加法注意[2]和点积（乘法）注意。点积注意与我们的算法相同，只是比例因子为1√dk。附加注意使用带有单个隐藏层的前馈网络计算兼容性函数。虽然两者在理论复杂性上相似，但由于可以使用高度优化的矩阵乘法代码来实现，因此在实践中，点积注意速度更快，空间效率更高。

而对于较小的dk值，这两种机制的表现类似，对于较大的dk值，加法注意优于点积注意[3]。我们怀疑，对于较大的dk值，点积的增长幅度较大，从而将softmax函数推到梯度非常小的区域4。为了抵消这种影响，我们将点积缩放1√dk。

### 3.2.2 多头注意力机制

我们发现，使用不同的学习线性投影将查询、键和值分别线性投影到dk、dk和dv维度，而不是使用dmodel维度的键、值和查询执行单个注意函数，这是有益的。在查询、键和值的每个投影版本上，我们并行执行注意函数，生成dv维输出值。如图2所示，这些值被连接并再次投影，从而得到最终值。

多头注意允许模型共同关注来自不同位置的不同表征子空间的信息。由于只有一个注意头，平均值会抑制这种情况。

![]()

其中投影为参数矩阵W Q i∈ Rdmodel×dk，wki∈ Rdmodel×dk，W V i∈ Rdmodel×dv和wo∈ Rhdv×d模型。

在这项工作中，我们采用了h=8个平行注意层或头部。对于每一个，我们使用dk=dv=dmodel/h=64。由于每个头部的维数减小，因此总的计算成本与全维单头部注意力的计算成本相似。

#### 3.2.3 注意力机制在我们模型中的应用

变压器以三种不同的方式使用多头注意：

- 在“编码器-解码器注意”层中，查询来自前一个解码器层，内存键和值来自编码器的输出。这允许解码器中的每个位置都参与输入序列中的所有位置。这模仿了序列到序列模型中典型的编码器-解码器注意机制，如[38,2,9]。
- 编码器包含自我注意层。在自我关注层中，所有键、值和查询都来自同一个位置，在本例中，是编码器中前一层的输出。编码器中的每个位置都可以关注编码器前一层中的所有位置。
- 类似地，解码器中的自我关注层允许解码器中的每个位置关注解码器中直到并包括该位置的所有位置。我们需要防止解码器中的信息向左流动，以保持自回归特性。我们通过屏蔽（设置为−∞) softmax输入中与非法连接对应的所有值。参见图2。

### 3.3 位置前馈网络

除了注意子层之外，编码器和解码器中的每一层都包含一个完全连接的前馈网络，该网络分别相同地应用于每个位置。这包括两个线性变换，中间有一个ReLU激活。

![]()

虽然线性变换在不同位置上是相同的，但它们在层与层之间使用不同的参数。另一种描述方法是将其描述为内核大小为1的两个卷积。输入和输出的维数为dmodel=512，内层的维数为dff=2048。

### 3.4 Embeddings and Softmax

与其他序列转换模型类似，我们使用学习到的嵌入将输入标记和输出标记转换为维度dmodel的向量。我们还使用通常学习的线性变换和softmax函数将解码器输出转换为预测的下一个令牌概率。在我们的模型中，我们在两个嵌入层和预softmax线性变换之间共享相同的权重矩阵，类似于[30]。在嵌入层中，我们将这些权重乘以√D模型。

### 3.5 位置编码

由于我们的模型不包含递归和卷积，为了使模型利用序列的顺序，我们必须注入一些关于表1的相对或绝对位置的信息：最大路径长度、每层复杂性和不同层类型的最小顺序操作数。n是序列长度，d是表示维，k是卷积的核大小，r是限制自我注意的邻域大小。

表1：不同层类型的最大路径长度、每层复杂性和最小顺序操作数。n是序列长度，d是表示维，k是卷积的核大小，r是限制自我注意的邻域大小。

![]()

序列中的标记。为此，我们在编码器和解码器堆栈底部的输入嵌入中添加“位置编码”。位置编码与嵌入具有相同的维度dmodel，因此可以将两者相加。有许多位置编码的选择，学习的和固定的[9]。

在这项工作中，我们使用不同频率的正弦和余弦函数：

![]()

其中pos是位置，i是尺寸。也就是说，位置编码的每个维度对应于一个正弦曲线。波长形成从2π到10000·2π的几何级数。我们之所以选择这个函数，是因为我们假设它可以让模型通过相对位置轻松学习参与，因为对于任何固定偏移量k，PEpos+k都可以表示为PEpos的线性函数。

我们还尝试使用学习的位置嵌入[9]，发现这两个版本产生了几乎相同的结果（见表3第（E）行）。我们选择正弦版本，因为它可能允许模型推断序列长度比训练期间遇到的序列长度更长。

## 4 为什么选择自注意力机制

在这一节中，我们将自关注层的各个方面与常用的用于映射符号长度表示（x1，…，xn）的可变长度序列映射到另一个等长序列（Z1，…，Zn）的递归和卷积层进行比较，其中有席，子。∈ Rd，如典型序列变换编码器或解码器中的隐藏层。激励我们使用自我关注，我们考虑三个愿望。

一个是每层的总计算复杂度。另一个是可以并行化的计算量，由所需的最小顺序操作数来衡量。

第三个是网络中长期依赖关系之间的路径长度。在许多序列转换任务中，学习长程依赖关系是一个关键挑战。影响学习此类依赖关系能力的一个关键因素是前向和后向信号在网络中必须穿过的路径长度。输入和输出序列中任何位置组合之间的路径越短，就越容易了解长期依赖关系[12]。因此，我们还比较了由不同层类型组成的网络中任意两个输入和输出位置之间的最大路径长度。

如表1所示，自我注意层通过恒定数量的顺序执行操作连接所有位置，而重复层需要O（n）个顺序操作。就计算复杂性而言，当序列长度n小于表示维度d时，自我注意层比重复层快，这通常是机器翻译中最先进模型使用的句子表示的情况，例如词段[38]和字节对[31]表示。为了提高涉及很长序列的任务的计算性能，自我关注可以限制为仅考虑以各自输出位置为中心的输入序列中大小为r的邻域。这会将最大路径长度增加到O（n/r）。我们计划在未来的工作中进一步研究这种方法。

内核宽度k<n的单个卷积层不能连接所有输入和输出位置对。在连续内核的情况下，这样做需要O（n/k）个卷积层的堆栈，或者在扩展卷积的情况下需要O（logk（n））个卷积层[18]，从而增加网络中任意两个位置之间的最长路径的长度。卷积层通常比循环层更昂贵，是k的一个因素。然而，可分离卷积[6]将复杂度大大降低到O（k·n·d+n·d2）。然而，即使在k=n的情况下，可分离卷积的复杂性也等于我们在模型中采用的自我注意层和点态前馈层的组合。

作为副作用，自我关注可以产生更多可解释的模型。我们从我们的模型中检查注意力分布，并在附录中展示和讨论示例。不仅个体的注意力集中者清楚地学会了执行不同的任务，许多人似乎表现出与句子的句法和语义结构相关的行为。

## 5 训练

本节介绍了我们模型的培训制度。

### 5.1 训练数据和批样本

我们在标准WMT 2014英语-德语数据集上进行了培训，该数据集由大约450万个句子对组成。句子使用字节对编码[3]进行编码，该编码拥有大约37000个令牌的共享sourcetarget词汇表。对于英语-法语，我们使用了更大的WMT 2014英语-法语数据集，该数据集包含3600万个句子，并将标记拆分为32000个单字词汇[38]。句子对按大致的序列长度分批排列在一起。每个训练批次包含一组句子对，包含大约25000个源标记和25000个目标标记。

### 5.2 硬件和时间表

我们使用8个NVIDIA P100 GPU在一台机器上训练我们的模型。对于使用本文所述超参数的基础模型，每个训练步骤大约需要0.4秒。我们对基础模型进行了总共100000步或12小时的培训。对于我们的大型号（如表3的底线所述），步进时间为1.0秒。大模型接受了300000步（3.5天）的训练。

### 5.3 优化器

我们使用了β1=0.9、β2=0.98和ϵ=10的Adam优化器[20]−9在培训过程中，我们根据以下公式改变了学习率：

![]()

这相当于线性增加第一个预热步骤训练步骤的学习率，然后按步骤数的平方反比成比例降低学习率。我们使用的预热步骤=4000。

### 5.4 正则化

我们在培训期间采用三种正规化：

剩余漏失我们将漏失[33]应用于每个子层的输出，然后再将其添加到子层输入并归一化。此外，我们对编码器和解码器堆栈中的嵌入和位置编码的和应用了dropout。对于基本模型，我们使用Pdrop=0.1的速率。

表2:Transformer在2014年英语-德语和英语-法语新闻测试中的BLEU分数比之前最先进的车型要高，而培训成本仅为其一小部分。

![]()

在训练期间，我们采用了值ϵls=0.1的标签平滑[36]。这伤害了困惑，因为模型学会了更加不确定，但提高了准确性和BLEU分数。

## 6 结论

### 6.1 机器翻译

在WMT 2014英语到德语翻译任务中，大变形金刚模型（表2中的变形金刚（big））比之前报道的最佳模型（包括合奏）的BLEU分数高出2.0以上，建立了28.4的最新BLEU分数。该型号的配置在表3的底线中列出。在8个P100 GPU上进行培训需要3.5天。即使是我们的基础模型也超过了之前发布的所有模型和集合，只是任何竞争模型培训成本的一小部分。

在WMT 2014英法翻译任务中，我们的大型模型的BLEU分数为41.0，优于之前发布的所有单一模型，培训成本不到之前最先进模型的1/4。为英语到法语培训的Transformer（大）模型使用了辍学率Pdrop=0.1，而不是0.3。

对于基本模型，我们使用了一个通过平均最后5个检查点获得的单一模型，这些检查点以10分钟的间隔写入。对于大型车型，我们平均了最后20个检查点。我们使用波束搜索，波束大小为4，长度惩罚α=0.6[38]。这些超参数是在开发集上进行实验后选择的。我们将推理期间的最大输出长度设置为输入长度+50，但尽可能提前终止[38]。

表2总结了我们的结果，并将我们的翻译质量和培训成本与文献中的其他模型架构进行了比较。我们通过乘以训练时间、使用的GPU数量和每个GPU 5的持续单精度浮点容量估计来估计用于训练模型的浮点操作数。

### 6.2 模型变化

为了评估变压器不同部件的重要性，我们以不同的方式改变了我们的基本模型，在开发集newstest2013上测量英语到德语翻译的性能变化。如前一节所述，我们使用了波束搜索，但没有检查点平均。我们在表3中给出了这些结果。

如第3.2.2节所述，在表3（A）行中，我们改变了注意头的数量以及注意键和值维度，保持计算量不变。虽然单头注意力比最佳设置差0.9个BLEU，但如果头太多，质量也会下降。

表3：变压器结构的变化。未列出的值与基础模型的值相同。所有指标都在英语到德语翻译开发集newstest2013上。根据我们的字节对编码，列出的困惑是每个单词的，不应与每个单词的困惑进行比较。

![]()

表4:Transformer很好地概括了英语选区解析（结果见《华尔街日报》第23节）

![]()

在表3（B）行中，我们观察到减少注意键大小dk会影响模型质量。这表明确定兼容性并不容易，而且比dot产品更复杂的兼容性功能可能是有益的。我们在第（C）行和第（D）行中进一步观察到，正如预期的那样，模型越大越好，而退出非常有助于避免过度拟合。在第（E）行中，我们将正弦位置编码替换为学习的位置嵌入[9]，并观察到与基本模型几乎相同的结果。

### 6.3 英语选区分析

为了评估转换器是否可以推广到其他任务，我们对英语选区分析进行了实验。这项任务提出了具体的挑战：输出受到强大的结构约束，并且大大长于输入。此外，RNN序列到序列模型无法在小数据区获得最先进的结果[37]。

我们在宾夕法尼亚州树状银行的《华尔街日报》（WSJ）部分[25]上训练了一个4层变压器，其dmodel=1024，大约有40K个训练句子。我们还在半监督的环境中对其进行训练，使用来自的更大的高置信度和BerkleyParser语料库，约1700万个句子[37]。对于仅WSJ设置，我们使用了16K令牌的词汇表，对于半监督设置，我们使用了32K令牌的词汇表。

我们仅进行了少量实验，以选择第22节发展集上的辍学者、注意力和剩余（第5.4节）、学习率和波束大小，所有其他参数在英语到德语基础翻译模型中保持不变。在推理过程中，我们将最大输出长度增加到输入长度+300。我们仅在WSJ和半监督设置中使用了21和α=0.3的波束大小。

我们在表4中的结果表明，尽管缺乏特定于任务的调整，我们的模型表现得出奇地好，产生了比所有以前报告的模型更好的结果，但递归神经网络语法除外[8]。

与RNN序列到序列模型[37]相比，Transformer的性能优于BerkeleyParser[29]，即使仅在华尔街日报40K句的训练集上进行训练也是如此。

## 7 总结

在这项工作中，我们提出了Transformer，这是第一个完全基于注意的序列转换模型，它用多头自我注意取代了编码器-解码器体系结构中最常用的递归层。

对于翻译任务，转换器的训练速度比基于循环层或卷积层的体系结构快得多。在WMT 2014英语到德语和WMT 2014英语到法语的翻译任务中，我们实现了新的技术水平。在前一个任务中，我们的最佳模型甚至比以前报道的所有集合都要好。

我们对基于注意力的模型的未来感到兴奋，并计划将其应用于其他任务。我们计划将转换器扩展到涉及输入和输出模式（而非文本）的问题，并研究本地的受限注意机制，以有效处理图像、音频和视频等大型输入和输出。减少一代人的顺序是我们的另一个研究目标。

我们用来训练和评估模型的代码可以在https://github.com/ tensorflow/Tensor2传感器。

感谢Nal Kalchbrenner和Stephan Gouws的富有成效的评论、更正和启发。

