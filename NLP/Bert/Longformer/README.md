# Longformer: The Long-Document Transformer 

# Longformer：处理长文本的Transformer

## 摘要

基于 Transformer 的模型由于其自注意力操作而**无法处理长序列**，该操作与序列长度成二次方缩放。为了解决这个限制，我们引入了带有注意力机制的 Longformer，该机制随序列长度线性扩展，从而可以轻松处理包含数千个或更长标记的文档。  Longformer 的注意力机制是标准自注意力的替代品，将**局部窗口注意力**与任务驱动的**全局注意力**相结合。 继之前在长序列转换器上的工作之后，我们在字符级语言建模上评估了 Longformer，并在 text8 和 enwik8 上取得了最先进的结果。与大多数先前的工作相比，我们还预训练 Longformer 并在各种下游任务上对其进行微调。我们预训练的 Longformer 在长文档任务上始终优于 RoBERTa，并在 WikiHop 和 TriviaQA 上设置了最新的最新结果。 我们最终介绍了 Longformer-Encoder-Decoder (LED)，这是一种支持长文档生成序列到序列任务的 Longformer 变体，并在 arXiv 摘要数据集上证明了其有效性。

## 1 前言

Transformers (Vaswani et al., 2017) 在包括生成语言建模 (Dai et al., 2019; Radford et al., 2019) 和判别语言理解在内的广泛自然语言任务中取得了最先进的结果 （德夫林等人，2019 年）。 这一成功部分归功于自注意力组件，它使网络能够从整个序列中捕获上下文信息。 虽然功能强大，但自注意力的内存和计算要求与序列长度成二次方增长，这使得处理长序列变得不可行（或非常昂贵）。

![]()

图 1：full self-attention 的运行时和内存以及 Longformer 的 self-attention 的不同实现；  Longformer-loop 是非矢量化的，Longformer-chunk 是矢量化的，而 Longformer-cuda 是自定义的 cuda 内核实现。Longformer 的内存使用与序列长度呈线性关系，这与当前 GPU 上长序列内存不足的完全自注意力机制不同。不同的实现速度不同，向量化的 Longformer-chunk 是最快的。 更多细节在第 3.2 节。

为了解决这个限制，我们提出了 Longformer，这是一种改进的 Transformer 架构，具有自注意力操作，可随序列长度线性扩展，使其在处理长文档时具有通用性（图 1）。 这对于自然语言任务（例如长文档分类、问答 (QA) 和共指解析）来说是一个优势，其中现有方法将长上下文划分或缩短为更小的序列，这些序列属于 BERT 式预训练模型的典型 512 个标记限制 . 这种分区可能会导致重要的跨分区信息的丢失，为了缓解这个问题，现有的方法通常依赖于复杂的架构来解决这种交互。 另一方面，我们提出的 Longformer 能够使用多层注意力构建整个上下文的上下文表示，从而减少对特定任务架构的需求。

最近的工作解决了 Transformer 在长序列上的计算效率低下的问题（见表 1）。 然而，他们主要关注自回归语言建模 (LM)，而将长文档转换器应用于迁移学习环境中的文档级 NLP 任务（Dai 和 Le，2015 年；Peters 等人，2018 年；Howard 和 Ruder，2018 年；Devlin 等人，2019 年）在很大程度上仍未得到探索。 我们解决了这个差距，并表明 Longformer 的注意力机制可以作为预训练 Transformer 中自我注意力机制的替代品，并在一系列文档 NLP 任务中获得收益。

Longformer 的注意力机制是窗口局部上下文自注意力和最终任务激发的全局注意力的组合，该全局注意力对任务进行归纳偏差编码。 通过**消融和对照试验**，我们表明两种注意力类型都是必不可少的——局部注意力主要用于构建上下文表示，而全局注意力允许 Longformer 构建用于预测的完整序列表示。

我们首先使用窗口化和新的扩张注意模式的组合在自回归字符级语言建模上评估 Longformer，允许模型在现代 GPU 上处理多达 32K 个字符的序列。 我们在 **text8 和 enwik8 基准数据集**上取得了最先进的结果，证明了 Longformer 在长文档建模中的有效性。

然后，为了评估 Longformer 替换现有预训练模型的完全自注意力操作的能力，我们使用掩码语言建模 (MLM) 目标对其进行预训练，从 RoBERTa (Liu et al., 2019) 发布的检查点继续。预训练后，我们通过微调将其应用于下游语言任务，并证明 Longformer 在广泛的文档级自然语言任务（包括文本分类、QA 和共指解析）上始终优于 RoBERTa，在以下两个方面取得了最先进的结果 这些数据集。

我们最后介绍了 Longformer 的一种变体，它不是仅编码器的 Transformer 架构，而是遵循类似于原始 Transformer 模型（Vaswani 等人，2017）的编码器-解码器架构，并且用于序列到序列（  seq2seq) 学习（Sutskever 等人，2014 年）。 我们将此模型称为 **Longformer-Encoder-Decoder (LED)**，它在编码器网络上使用 Longformer 的高效注意力模式，使其能够处理长文档 seq2seq 任务，例如摘要。 我们证明了 LED 在 arXiv 摘要数据集上的有效性（Cohan 等人，2018 年）。

![]()

表 1：先前关于为长文档调整 Transformer 的工作总结。  ltr：从左到右。

## 2 相关工作

**Long-Document Transformer** 图 1 总结了近期关于长文档的工作。 已经探索了两种类型的自注意力方法。 第一种是从左到右 (ltr) 方法，它以从左到右移动的块处理文档。 尽管此类模型在自回归语言建模中取得了成功，但它们不适用于具有受益于双向上下文的任务的迁移学习方法。

我们的工作属于另一种通用方法，该方法定义了某种形式的稀疏注意力模式并避免计算完整的二次注意力矩阵乘法。 与我们的注意力模式最相似的模型是 Sparse Transformer (Child et al., 2019)，它使用 BlockSparse (Gray et al., 2017) 提供的大小为 8x8 的块的扩张滑动窗口形式。 我们的实现（第 3 节）还包括一个自定义 CUDA 内核，但它比 BlockSparse 更灵活和可维护，后者是用 C++ 实现的，专为 TensorFlow 的特定版本而设计。 我们还介绍了适用于常见 NLP 任务（§3）的额外任务驱动全局注意力模式，并表明它们对于迁移学习环境中的良好表现至关重要。

![]()

图 2：比较 Longformer 中的完整自注意力模式和注意力模式的配置。

一些模型尝试了自回归语言建模以外的任务，这是向前迈出的一步，因为可以说专注于语言建模作为主要评估导致了适用性有限的模型的开发。  BPTransformer (Ye et al., 2019) 对机器翻译 (MT) 进行了评估，但没有探索 pretrainfinetune 设置。  Blockwise attention (Qiu et al., 2019) 预训练了他们的模型并评估了问答 (QA)。 然而，评估是有限的，因为它不包括语言建模，而且 QA 数据集的文档相对较短，2 因此该模型在长文档任务上的有效性仍有待探索。

**Task-specific Models for Long Documents** 已经开发了许多特定于任务的方法来解决 BERT 等预训练 Transformer 模型的 512 限制。 最简单的方法只是截断文档，通常用于分类（Xie 等，2019）。另一种方法将文档分成长度为 512 的块（可能重叠），分别处理每个块，然后将激活与特定于任务的模型相结合（Joshi 等，2019）。在多跳和开放域 QA 任务中流行的第三种方法使用两阶段模型，其中第一阶段检索传递到第二阶段进行答案提取的相关文档（克拉克和加德纳，2017 年；陈等人，2017 年）。由于两阶段方法的截断或级联错误，所有这些方法都会遭受信息丢失。 相比之下，Longformer 可以在不截断或分块的情况下处理长序列，这使我们能够采用更简单的方法，将可用上下文连接起来并在一次传递中对其进行处理。

一些同时代的作品 3 探索了与 Longformer 类似的想法，在 Transformer 中使用局部 + 全局注意力，并针对长文档自然语言任务对其进行预训练。 特别是，ETC（Ainslie 等人，2020 年）使用类似的局部 + 全局注意力而不是完全自我注意力来将 Transformer 扩展到长文档。 与 Longformer 不同，ETC 使用相对位置嵌入（我们仅用于自回归 LM 设置），为预训练引入了额外的训练目标（CPC 损失），并以略有不同的方式配置全局注意力。
   它在包括阅读理解和分类在内的多项任务上显示出强大的结果。  GMAT（Gupta 和 Berant，2020 年）使用了类似的想法，即输入中的几个全局位置用作全局记忆。  BigBird (Zaheer et al., 2020) 是对 ETC 的扩展，对附加任务进行评估，包括总结。 重要的是，通过理论分析，BigBird 表明稀疏 Transformer 是序列函数的通用逼近器，并保留了完全自注意力的这些属性。

## 3 Longformer

原始的 Transformer 模型有一个自注意力组件，时间和内存复杂度为 O(n2)，其中 n 是输入序列长度。 为了应对这一挑战，我们根据“注意模式”来稀疏化完整的自注意矩阵，该“注意模式”指定了相互注意的输入位置对。与完全自注意力不同，我们提出的注意力模式与输入序列线性缩放，使其对更长的序列有效。 本节讨论这种注意力模式的设计和实现。

### 3.1 注意模式

**滑动窗口** 考虑到局部上下文的重要性（Kovaleva 等人，2019 年），我们的注意力模式在每个标记周围采用固定大小的窗口注意力。 使用这种窗口注意力的多个堆叠层会产生一个大的感受野，其中顶层可以访问所有输入位置，并有能力构建包含整个输入信息的表示，类似于 CNN（Wu 等人，2019）。给定一个固定的窗口大小 w，每个令牌在每侧处理 1 2w 个令牌（图 2b）。 该模式的计算复杂度为 O(n × w)，它与输入序列长度 n 线性缩放。在具有 ℓ 层的转换器中，顶层的感受野大小为 ℓ × w（假设 w 对于所有层都是固定的）。 根据应用程序，为每一层使用不同的 w 值来平衡效率和模型表示能力（第 4.1 节）可能会有所帮助。

**Dilated Sliding Window** 为了在不增加计算量的情况下进一步增加感受野，可以“扩张”滑动窗口。 这类似于扩张的 CNN（van den Oord 等人，2016 年），其中窗口具有大小为 d 的间隙（图 2c）。假设所有层的 d 和 w 固定，感受野为 ℓ × d × w，即使 d 的值很小，它也可以达到数万个标记。

在多头注意力中，每个注意力头计算不同的注意力分数。 我们发现每个头部具有不同扩张配置的设置通过允许一些没有扩张的头部专注于局部上下文，而其他具有扩张的头部专注于更长的上下文来提高性能。

**全局注意力 **在用于自然语言任务的最先进的 BERT 样式模型中，最佳输入表示与语言建模不同，并且因任务而异。 对于掩码语言建模 (MLM)，该模型使用本地上下文来预测掩码词，而对于分类，该模型将整个序列的表示聚合为一个特殊的标记（在 BERT 的情况下为 [CLS]）。 对于 QA，问题和文档是串联的，允许模型通过自注意力将问题与文档进行比较。

在我们的例子中，窗口化和扩张的注意力不够灵活，无法学习特定于任务的表示。 因此，我们在几个预先选择的输入位置上添加了“全局注意力”。 重要的是，我们使这个注意力操作对称：也就是说，具有全局注意力的令牌关注整个序列中的所有令牌，并且序列中的所有令牌都关注它。 图 2d 显示了一个滑动窗口注意力的例子，在自定义位置的几个标记上具有全局注意力。 例如，对于分类，全局注意力用于 [CLS] 标记，而在 QA 中，全局注意力用于所有问题标记。 由于此类标记的数量相对于 n 而言很小，并且与 n 无关，因此结合局部和全局注意力的复杂度仍然为 O(n)。 虽然指定全局注意力是特定于任务的，但它是一种向模型注意力添加归纳偏差的简单方法，并且比现有的任务特定方法简单得多，后者使用复杂的架构来组合较小输入块的信息。

全局注意力的线性投影回想一下，给定线性投影 Q、K、V，Transformer 模型（Vaswani 等人，2017 年）计算注意力分数如下：

![]()

我们使用两组投影 Qs、Ks、Vs 来计算滑动窗口注意力的注意力分数，以及 Qg、Kg、Vg 来计算全局注意力的注意力分数。 额外的投影为模拟不同类型的注意力提供了灵活性，我们表明这对于下游任务的最佳性能至关重要。  Qg、Kg、Vg 均使用与 Qs、Ks、Vs 匹配的值进行初始化。

### 3.2 实现

在常规转换器中，注意力分数的计算方式与公式相同。  1. 代价高昂的操作是矩阵乘法 QKT，因为 Q 和 K 都有 n（序列长度）投影。 对于 Longformer，扩张的滑动窗口注意力只计算 QKT 的固定数量的对角线。 如图 1 所示，与完全自注意力的二次增加相比，这导致内存使用量的线性增加。 但是，实现它需要一种带状矩阵乘法形式，而现有的深度学习库（如 PyTorch/Tensorflow）不支持这种形式。 图 1 比较了三种不同实现方式的性能：loop 是一种内存高效的 PyTorch 实现，支持扩张但速度慢得无法使用，仅用于测试；  chunks 只支持非扩张情况，用于预训练/微调设置；  cuda 是我们使用 TVM (Chen et al., 2018) 实现的功能齐全的高度优化的自定义 CUDA 内核，用于语言建模实验（有关更多详细信息，请参见附录 A）。

## 4 自回归语言建模

自回归或从左到右的语言建模被松散地定义为在给定输入序列中的先前标记/字符的情况下估计现有标记/字符的概率分布。此任务被认为是自然语言中的基本任务之一，最近使用转换器对长序列进行建模的先前工作依赖于此任务作为其主要评估（Dai 等人，2019 年；Rae 等人，2020 年；Sukhbaatar 等人，2019 年）。  , 2019)。同样，我们在自回归语言建模上开发和评估我们的模型。

### 4.1 注意力模式

对于自回归语言建模，我们使用我们的扩张滑动窗口注意力。继 Sukhbaatar 等人之后。  (2019) 我们在各层中使用不同的窗口大小。 特别是，我们对较低层使用较小的窗口大小，并在移动到较高层时增加窗口大小。这允许顶层学习整个序列的更高级别表示，同时让较低层捕获本地信息。 此外，它提供了效率（较小的窗口尺寸由于较少的非零值而计算成本较低）和性能（较大的窗口尺寸具有更丰富的表示能力并通常会导致性能改进）之间的平衡。

我们不对较低层使用扩张的滑动窗口，以最大限度地提高其学习和利用直接本地上下文的能力。 对于更高的层，我们仅在 2 个头上使用少量的递增膨胀。 这使模型能够在不牺牲本地上下文的情况下直接处理远处的标记。

### 4.2 实验设置

为了与之前的工作进行比较，我们专注于字符级 LM（text8 和 enwik8；Mahoney，2009）。

**训练** 理想情况下，我们希望在现代 GPU 内存中可以容纳的最大窗口大小和序列长度上训练我们的模型。 然而，我们发现模型需要大量的梯度更新来首先学习局部上下文，然后再学习利用更长的上下文。 为了适应这一点，我们采用了分阶段训练过程，在该过程中我们增加了多个训练阶段的注意力窗口大小和序列长度。 特别是，在第一阶段，我们从较短的序列长度和窗口大小开始，然后在每个后续阶段，我们将窗口大小和序列长度加倍，并将学习率减半。 这使得训练速度更快，同时将慢速部分（最长序列和窗口大小）保持到最后。
   我们在 5 个总阶段上训练模型，最后一个阶段的起始序列长度为 2,048，结束序列长度为 23,040（有关每个阶段的详细配置以及所有其他超参数，请参见附录 B）。

![]()

![]()

