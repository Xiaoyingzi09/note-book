# Graph Neural Networks for Natural Language Processing: A Survey 

# 用于自然语言处理的图神经网络：综述

## 摘要

深度学习已成为应对自然语言处理 (NLP) 中各种任务的主要方法。 尽管文本输入通常表示为一系列标记，但有很多 NLP 问题可以用图结构最好地表达。 因此，人们对为大量 NLP 任务开发新的图深度学习技术的兴趣激增。 在本次调查中，我们全面概述了用于自然语言处理的图神经网络 (GNN)。 我们提出了一种新的 NLP GNN 分类法，它沿三个轴系统地组织了 NLP 的 GNN 现有研究：图构建、图表示学习和基于图的编码器-解码器模型。 我们进一步介绍了大量利用 GNN 功能的 NLP 应用程序，并总结了相应的基准数据集、评估指标和开源代码。 最后，我们讨论了充分利用 GNN 进行 NLP 的各种突出挑战以及未来的研究方向。 据我们所知，这是对用于自然语言处理的图神经网络的第一个全面概述。

关键词：图神经网络，自然语言处理，图深度学习

## 1 前言

深度学习已成为当今自然语言处理 (NLP) 中处理各种任务的主要方法，尤其是在对大规模文本语料库进行操作时。 传统上，文本序列在 NLP 任务中被认为是一包令牌，例如 BoW 和 TF-IDF。 随着 Word Embeddings 技术（Mikolov 等人，2013 年；Pennington 等人，2014 年）的近期成功，句子在 NLP 任务中通常表示为一系列标记。 因此，流行的深度学习技术，例如循环神经网络（Schuster 和 Paliwal，1997）和卷积神经网络（Krizhevsky 等，2012）已广泛应用于文本序列建模。

然而，有各种各样的 NLP 问题可以用图结构最好地表达。 例如，文本序列中的句子结构信息（即句法分析树，如依赖和选区分析树）可以通过结合特定任务的知识来增加原始序列数据。 类似地，序列数据中的语义信息（即语义解析图，如抽象含义表示图和信息提取图）也可以用来增强原始序列数据。 因此，这些图结构数据可以编码实体标记之间复杂的成对关系，以学习更多信息表示。

不幸的是，由于图数据的复杂性，例如不规则结构和不同大小的节点，对欧几里德数据（例如图像）或序列数据（例如文本）具有破坏性的深度学习技术不能立即适用于图结构数据 邻居。 因此，这一差距推动了图深度学习研究的潮流，尤其是图神经网络 (GNN) 的开发（Kipf 和 Welling，2016 年；Defferrard 等人，2016 年；Hamilton 等人，2017a）。

这波图深度学习和 NLP 交叉领域的研究浪潮影响了各种 NLP 任务。 人们对应用和开发不同 GNN 变体的兴趣激增，并在许多 NLP 任务中取得了相当大的成功，包括句子分类等分类任务（Henaff 等人，2015 年；Huang 和 Carley，2019 年）、语义角色标记罗和 赵 (2020); 桂等人。  (2019) 和关系抽取 (Qu et al., 2020; Sahu et al., 2019)，生成机器翻译等任务 (Bastings et al., 2017; Beck et al., 2018)、问题生成 (Pan et al., 2018)  al., 2020; Sachan et al., 2020) 和总结 (Fernandes et al., 2019; Yasunaga et al., 2017)。 尽管这些现有研究取得了成功，但 NLP 图的深度学习仍然遇到许多挑战，即，

- 自动将原始文本序列数据转换为高度图结构化的数据。这种挑战在 NLP 中是深远的，因为大多数 NLP 任务都涉及使用文本序列作为原始输入。 从文本序列自动构建图以利用底层结构信息是利用图神经网络解决 NLP 问题的关键步骤。
- 正确确定图表示学习技术。 提出专门设计的 GNN 来学习不同图结构数据的独特特征（例如无向图、有向图、多关系图和异构图）至关重要。
- 对复杂数据进行有效建模。 这种挑战很重要，因为许多 NLP 任务涉及学习基于图的输入与其他高度结构化的输出数据（例如序列、树以及节点和边的多种类型的图数据）之间的映射。

在本次调查中，我们将首次全面概述用于自然语言处理的图神经网络。 我们的调查对机器学习和 NLP 社区都是及时的，它涵盖了相关且有趣的主题，包括 NLP 的自动图构建、NLP 的图表示学习、各种基于 GNN 的高级编码器-解码器模型（即 graph2seq、graph2tree 和 graph2graph） 用于 NLP，以及 GNN 在各种 NLP 任务中的应用。 我们强调我们的主要贡献如下：

- 我们提出了一种新的 NLP GNN 分类法，它沿三个轴系统地组织了 NLP 的 GNN 现有研究：图构建、图表示学习和基于图的编码器-解码器模型。
- 我们对各种 NLP 任务的最先进的基于 GNN 的方法进行了最全面的概述。 我们提供了基于领域知识和语义空间的各种图构建方法的详细描述和必要的比较，针对各种类型的图结构数据的图表示学习方法，基于 GNN 的编码器 - 解码器模型给出了不同的输入和输出数据类型组合。
- 我们介绍了大量利用 GNN 强大功能的 NLP 应用程序，包括它们如何沿着三个关键组件（即图构建、图表示学习和嵌入初始化）处理这些 NLP 任务，以及提供相应的基准数据集， 评估指标和开源代码。
- 我们概述了充分利用 GNN 进行 NLP 的各种突出挑战，并为富有成果和未探索的研究方向提供讨论和建议。

调查的其余部分结构如下。 第 2 节从图的角度回顾了 NLP 问题，然后简要介绍了一些具有代表性的传统基于图的解决 NLP 问题的方法。 第 3 节详细阐述了图神经网络的基本基础和方法，图神经网络是一类直接对图结构数据进行操作的现代神经网络。 我们还提供了整个调查中使用的符号列表。 第 4 节重点介绍两种主要的图构建方法，即静态图构建和动态图构建，用于在各种 NLP 任务中构建图结构化输入。 第 5 节讨论了各种图表示学习技术，这些技术直接在为各种 NLP 任务构建的图上进行操作。 第 6 节首先介绍了典型的 Seq2Seq 模型，然后讨论了用于 NLP 任务的两种典型的基于图的编码器-解码器模型（即图到树和图到图模型）。 第 7 节讨论了 12 个使用 GNN 的典型 NLP 应用程序，同时提供了所有应用程序及其子任务、评估指标和开源代码的摘要。 第 8 节讨论了用于 NLP 的 GNN 的各种一般挑战，并指出了未来的研究方向。最后，第 9 节总结了本文。 分类法沿三个轴系统地组织了用于 NLP 方法的 GNN：图构建、图表示学习、编码器解码器模型，其应用如图 1 所示。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/Graph&NLP/%E7%BB%BC%E8%BF%B0/figure/figure_1.png)

图 1：分类法，它沿三个轴系统地组织 NLP 的 GNN：图构建、图表示学习、编码器-解码器模型和应用程序。

## 2 基于图的自然语言处理算法

在本节中，我们将首先从图的角度回顾 NLP 问题，然后简要介绍一些具有代表性的传统基于图的解决 NLP 问题的方法。

#### 2.1 自然语言处理：图视角

我们表示自然语言的方式反映了我们对其的特定观点，并对我们处理和理解它的方式产生了根本性的影响。 通常，存在三种不同的表示自然语言的方式。 最简化的方法是将自然语言表示为一袋标记。 这种自然语言的观点完全忽略了标记在文本中出现的具体位置，而只考虑唯一标记在文本中出现的次数。 如果随机打乱给定的文本，从这个角度来看，文本的含义根本不会改变。 采用这种观点的最具代表性的 NLP 技术是主题建模 (Blei et al., 2003)，其目的是将每个输入文本建模为主题的混合，其中每个主题可以进一步建模为单词的混合。

一种更自然的方法是将自然语言表示为一系列标记。 这就是人类通常说和写自然语言的方式。 与上述袋子视角相比，这种自然语言视图能够捕获更丰富的文本信息，例如，哪些两个标记是连续的，以及一个词对在本地上下文中共出现多少次。 持这种观点的最具代表性的 NLP 技术包括在预测中实现顺序依赖的线性链 CRF（Lafferty 等人，2001 年），以及通过预测上下文来学习词嵌入的 word2vec（Mikolov 等人，2013 年） 目标词的词。

第三种方式是将自然语言表示为图。 图在 NLP 中无处不在。 虽然将文本视为序列数据可能是最明显的，但在 NLP 社区中，将文本表示为各种图形的历史悠久。 文本或世界知识的常见图表示包括依赖图、选区图、AMR 图、IE 图、词汇网络和知识图。 此外，还可以构建包含文档、段落、句子和单词等元素的多个层次结构的文本图。 与上述两种观点相比，这种自然语言观点能够捕捉到更丰富的文本元素之间的关系。 正如我们将在下一节介绍的那样，许多传统的基于图的方法（例如，随机游走、标签传播）已成功应用于具有挑战性的 NLP 问题，包括词义消歧、名称消歧、共参考解析、情感分析和文本 聚类。

#### 2.2 基于图的自然语言处理方法

在前一小节中，我们讨论了许多 NLP 问题可以自然地转化为基于图的问题。 在本小节中，我们将介绍已成功应用于 NLP 应用程序的各种经典的基于图的算法。 具体来说，我们将首先简要说明一些具有代表性的基于图的算法及其在 NLP 领域的应用。 然后我们进一步讨论它们与 GNN 的联系。 对于 NLP 的传统基于图的算法的全面覆盖，我们建议读者参考（Mihalcea 和 Radev，2011）。

##### 2.2.1 随机行走算法

方法随机游走是一类基于图的算法，可在图中生成随机路径。 为了进行随机游走，可以从图中的任何节点开始，并根据一定的转移概率重复选择每次访问随机相邻节点。 随机游走中所有访问过的节点然后形成一条随机路径。 随机游走收敛后，可以获得图中所有节点的平稳分布，该分布可用于通过对概率分数进行排序或测量两个图的相关性来选择具有高结构重要性的图中最显着的节点 通过计算两个随机游走分布之间的相似性。

应用 随机游走算法已应用于各种 NLP 应用，包括文本语义相似性的度量（Ramage 等人，2009 年）和语义网络上的语义距离（Hughes 和 Ramage，2007 年）、词义消歧（Mihalcea，2005 年；Tarau 等，2005）、名称消歧（Minkov 等，2006）、查询扩展（Collins-Thompson 和 Callan，2005）、关键字提取（Mihalcea 和 Tarau，2004）和跨语言信息检索（Monz 和 Dorr  , 2005)。 例如，给定一个语义网络和一个词对，Hughes 和 Ramage (2007) 使用随机游走算法计算了特定于词的平稳分布，并测量了两个词之间的距离，作为该图上随机游走分布之间的相似度， 偏向于给定词对中的每个输入词。 为了解决电子邮件数据的名称消歧任务，Minkov 等人。  (2006) 从电子邮件语料库中构建了电子邮件特定项目（例如，发件人、收件人和主题）的图，并提出了一种“惰性”主题敏感随机游走算法，该算法引入了随机游走将在某个时间点停止的概率。 给定节点。 给定电子邮件图和输入电子邮件中出现的模糊名称，随机游走会偏向于给定电子邮件的文本，并通过选择静止状态中得分最高的人员节点将名称解析为正确的引用 收敛后分布。 为了解决关键字提取任务，Mihalcea 和 Tarau (2004) 提出在单词的共现图上执行随机游走，并根据它们在平稳分布中的概率分数对文本中的单词的重要性进行排序。

##### 2.2.2 图聚类算法

方法常见的图聚类算法包括谱聚类、随机游走聚类和最小割聚类。 谱聚类算法利用图的拉普拉斯矩阵的谱（特征值）在使用现有算法（如 K 均值）进行聚类之前执行降维。 随机游走聚类算法通过在图上进行 t 步随机游走来进行操作，因此，每个节点都被表示为一个概率向量，表示图中所有其他节点的 t 步生成概率。 任何聚类算法都可以应用于生成链接向量。 请注意，出于图聚类的目的，更优选较小的 t 值，因为我们更感兴趣的是捕获局部结构信息而不是全局结构信息（由随机游走收敛后的平稳分布编码）。 最小割算法也可用于将图划分为簇。

应用图聚类算法已成功应用于解决文本聚类任务。 例如，Erkan (2006) 提出使用从有向生成图（包含 n 个文档节点）上的 t 步随机游走导出的 n-dim 概率分布作为语料库中每个文档的向量表示。 然后，图聚类算法可以使用这些文档表示来生成文档集群。 请注意，生成图是通过按照 Ponte 和 Croft (1998) 提出的语言模型方法计算语料库中每个有序文档对的生成概率来构建的。

##### 2.2.3 图匹配算法

方法 图匹配算法旨在计算两个图之间的相似性。 其中，Graph Edit Distance是最常用的衡量两个图相异度的方法。 它将距离计算为将一个图转换为另一个图所需的更改次数（即添加、删除、替换）。 然后可以将相异性分数转换为相似性分数。

应用图匹配算法在文本蕴涵任务中有应用，该任务旨在决定是否可以从文本中推断出给定的句子。 例如，Haghighi 等人。  (2005) 假设当假设图与文本图的匹配成本较低时，假设是从文本中蕴涵的，因此应用了图匹配算法来解决该问题。

##### 2.2.4 标签传播算法

方法 标签传播算法 (LPA) 是一类基于半监督图的算法，可将标签从标记数据点传播到以前未标记的数据点。 基本上，LPA 通过在图中迭代地传播和聚合标签来运行。 在每次迭代中，每个节点都会根据其相邻节点拥有的标签更改其标签。 结果，标签信息在图形中扩散。

应用 LPA 已广泛用于网络科学文献中，用于发现复杂网络中的社区结构。 在 NLP 的文献中，LPA 已成功应用于词义消歧（Niu et al., 2005）和情感分析（Goldberg and Zhu, 2006）。 这些应用程序通常专注于标记数据稀缺的半监督学习设置，并利用 LPA 算法将标签从有限的标记示例传播到大量类似的未标记示例，并假设相似的示例应该具有相似的标签。

##### GNNS 的限制和连接

尽管传统的基于图的算法已成功应用于许多 NLP 任务，但它们有一些局限性。 首先，它们的表达能力有限。 他们主要关注捕获图的结构信息，但没有考虑对许多 NLP 应用程序也非常重要的节点和边缘特征。 其次，传统的基于图的算法没有统一的学习框架。 不同的基于图的算法具有非常不同的属性和设置，并且仅适用于某些特定用例。

传统基于图的算法的上述局限性需要一个统一的基于图的学习框架，在对图结构和节点/边属性进行建模时具有强大的表达能力。 最近，GNN 作为一种特殊的神经网络，可以对任意图形结构的数据进行建模，受到越来越多的关注。 大多数 GNN 变体可以被视为基于消息传递的学习框架。 与传统的基于消息传递的算法（如 LPA）通过在图上传播标签来运行不同，GNN 通常通过通过多个神经层转换、传播和聚合节点/边缘特征来运行，以便学习更好的图表示。 作为通用的基于图的学习框架，GNN 可以应用于各种与图相关的任务，例如节点分类、链接预测和图分类。

## 3 图神经网络

在前一章中，我们已经说明了用于不同 NLP 应用程序的各种传统的基于图的方法。 在本章中，我们将详细阐述图神经网络 (GNN) 的基本基础和方法，这是一类直接对图结构数据进行操作的现代神经网络。 为了便于描述这些技术，我们在表 1 中列出了本次调查中使用的所有符号，其中包括图神经网络和 NLP 领域中的变量和操作。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/Graph&NLP/%E7%BB%BC%E8%BF%B0/figure/table_1.png)

表 1：符号

#### 3.1 基础

图神经网络本质上是图表示学习模型，可以应用于以节点为中心的任务和以图为中心的任务。  GNN 学习图中每个节点的嵌入并聚合节点嵌入以生成图嵌入。 通常，节点嵌入的学习过程利用输入节点嵌入和图结构，可以概括为：

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/Graph&NLP/%E7%BB%BC%E8%BF%B0/figure/formula_1.png)

其中 A ∈ Rn×n 是图的邻接矩阵， H(l−1) = {h(l−1) 1 , h(l−1) 2 , ..., h(l−1) n }  ∈ Rn×d 表示在 l − 1-th GNN 层的输入节点嵌入，H(l) 是更新后的节点嵌入。  d 是 h(l−1) i 的维度。 我们将等式（1）中描述的过程称为图过滤，将 ffilter(·, ·) 称为图过滤器。 特定模型的区别仅在于 ffilter(·,·) 的选择和参数化方式。 图过滤不会改变图的结构，但会细化节点嵌入。 图过滤层堆叠到 L 层以生成最终节点嵌入。

由于图过滤不会改变图结构，因此引入池化操作来聚合节点嵌入以生成受 CNN 启发的图级嵌入。 在 GNN 模型中，图池化将图及其节点嵌入作为输入，然后生成具有较少节点的较小图及其对应的新节点嵌入。 图池化操作可以总结如下：

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/Graph&NLP/%E7%BB%BC%E8%BF%B0/figure/formula_2.png)

其中 fpool(·, ·) A ∈ Rn×n 和 A' ∈ Rn'×n' 是图池化前后的邻接矩阵。  H ∈ Rn×d 和 H′ ∈ Rn′×d′ 是图池化前后的节点嵌入。 在大多数情况下，n' 设置为 1 以获得整个图的嵌入。

#### 3.2 方法论

##### 3.2.1 图滤波器

等式（1）中存在多种图过滤器 f 的实现，大致可以分为基于谱的图过滤器、基于空间的图过滤器、基于注意力的图过滤器和基于循环的图过滤器。 从概念上讲，基于谱的图滤波器基于谱图理论，而基于空间的方法使用其在图上的空间近邻节点来计算节点嵌入。 一些基于光谱的图过滤器可以转换为基于空间的图过滤器。 基于注意力的图过滤器受到自注意力机制（Vaswani et al., 2017）的启发，为不同的邻居节点分配不同的注意力权重。 基于循环的图过滤器引入了门控机制，并且模型参数在不同的 GNN 层之间共享。 接下来，我们将通过介绍它们的一些代表性 GNN 模型来详细解释这四种类型的图过滤器。

**基于频域的图形过滤器** 基于光谱的图过滤器的典型示例是图卷积网络 (GCN)（Kipf 和 Welling，2016 年）。 图上的谱卷积定义为信号 xi ∈ Rn（节点 vi 的标量）与傅立叶域中由 θ ∈ Rn 参数化的滤波器 ffilter = diag(θ) 的乘法：

![]()

其中 U 是归一化图拉普拉斯算子 L = In −D− 1 2 AD− 1 2 的特征向量矩阵。  In 是单位矩阵，D 是度矩阵，Λ 是 L 的特征值。

然而，完整特征分解的计算非常昂贵。 为了解决这个问题，Defferrard 等人。  (2016) 使用切比雪夫多项式 Tp(x) 到 P 阶的截断扩展来近似 gθ(Λ)。 等式 (3) 可以表示为：

![]()

其中 ~L = 2 λmax L − In。  λmax 是 L 的最大特征值。θ′ k ∈ RP 是切比雪夫系数的向量。  Chebyshev 多项式可以递归定义：Tk(xi) = 2xiTk−1(xi) − Tk−2(xi)，其中 T0(xi) = 1 和 T1(xi) = xi。 等式（4）是拉普拉斯算子中的第 K 阶多项式，它表明每个中心节点仅依赖于 P 跳范围内的节点。

因此，基于图卷积的神经网络模型可以使用方程（1）来堆叠多个卷积层。  (4). 通过将逐层卷积操作限制为 P = 1 并堆叠多个卷积层，Kipf 和 Welling (2016) 提出了多层图卷积网络 (GCN)。 它进一步逼近 λmax ≈ 2 和方程。  (4) 简化为：

![]()

有两个自由参数 θ′ 0 和 θ′ 1 。为了缓解过拟合问题并最小化操作次数（如矩阵乘法），通过设置单个参数 θ = θ′ 0 来约束参数数量是有益的 = -θ′ 1：

![]()

重复应用此算子可能会导致数值不稳定和爆炸/消失梯度，Kipf 和 Welling (2016) 提出使用重整化技巧：In + D− 1 2 AD− 1 2 → ˜D− 1 2 ˜A ˜D− 1  2 ，其中 ˜A = A + In 且 ˜Dii = � j ˜Aij。 最后，该定义可以用具有 d 个输入通道（即每个节点的 d 维特征向量）和 F 个滤波器或特征映射的信号 H ∈ Rn×d 概括如下：

![]()

这里，W(l-1) 是特定层的可训练权重矩阵，σ(·) 表示激活函数。H(l) ∈ Rn×d 是第 (l − 1) 层的激活节点嵌入。

**基于空域的图滤波器**类似于传统 CNN 的卷积操作，基于空间的图过滤器根据节点的空间关系操作图卷积。 基于空间的图过滤器通过将目标节点的表示与其邻居的表示进行卷积来导出目标节点的更新表示。 另一方面，基于空间的图过滤器持有信息传播的思想，即消息传递。 基于空间的图卷积操作本质上是将节点信息作为消息沿边传播。 在这里，我们介绍两种基于空间图过滤器的典型 GNN，分别是消息传递神经网络 (MPNN)（Gilmer 等人，2017）和 GraphSage（Hamilton 等人，2017a）。

MPNN (Gilmer et al., 2017) 提出了一个基于空间的图过滤器 ffilter 的通用框架，它是一个由 fU 和 fM 组成的复合函数。 它将图卷积视为一种消息传递过程，其中信息可以直接沿着边从一个节点传递到另一个节点。  MPNN 运行 K 步消息传递迭代，让信息进一步传播到 Khop 相邻节点。 目标节点 vi 上的消息传递函数，即基于空间的图过滤器定义为

![]()

其中 h(0) i = xi，fU(·) 和 fM(·) 分别是具有可学习参数的更新和消息聚合函数。 在导出每个节点的隐藏表示后，可以将 h(L) i（L 是图卷积层的数量）传递给输出层以执行节点级预测任务或传递给读出函数以执行图级预测任务 .  MPNN 非常通用，可以通过应用 fU(·) 和 fM(·) 的不同函数来包含许多现有的 GNN。

考虑到一个节点的邻居数量可以从一个到一千个甚至更多，在具有数百万个节点的巨图中获取节点邻域的完整大小是低效的。  GraphSage (Hamilton et al., 2017a) 采用采样为每个节点获取固定数量的邻居作为

![]()

其中 N(vi) 是节点 vi 的相邻节点的随机样本。 聚合函数可以是对节点排序的排列不变的任何函数，例如均值、总和或最大值操作。

**基于注意力的图过滤器** 原始版本的 GNN 将输入图的边连接视为固定的，并且在图学习过程中不会动态调整连接信息。 受上述观察的启发，并受到多头注意力机制在 Transformer 模型（Vaswani 等人，2017 年；Velickovic 等人，2018 年）中的成功应用的启发，通过引入多头注意力网络（GAT）提出了 GNN 架构的头部注意力机制能够在执行消息传递时动态学习边缘的权重（即注意力分数）。 更具体地说，当为图中的每个目标节点聚合来自相邻节点的嵌入时，多头注意力机制将考虑目标节点与每个相邻节点之间的语义相似性，并且重要的相邻节点将被分配更高的注意力分数，当 执行邻域聚合。 对于第 l 层，GAT 因此使用以下注意力机制的公式，

![]()

其中⃗u(l)和⃗W(l)分别是第l层的权重向量和权重矩阵，|| 是向量连接操作。 请注意，N(vi) 是 vi 的 1 跳邻域，包括其自身。 在获得每对节点 vi 和 vj 的注意力分数 αij 后，更新的节点嵌入可以计算为输入节点特征的线性组合，后跟一些非线性σ，公式为：

![]()

为了稳定上述self-attention的学习过程，受到Vaswani等人的启发。(2017)，采用了多个独立的自注意力机制，并将它们的输出连接起来以产生以下节点嵌入：

![]()

而最终的 GAT 层（即具有 L 层的 GNN 的第 L 层）采用平均而不是串联来组合多头注意力输出。

![]()

**基于循环的图过滤器**基于循环的图过滤器的典型示例是门控图神经网络 (GGNN) 过滤器。 从典型的 GNN 到 GGNN 的最大改进是使用了门控循环单元 (GRU)（Cho 等人，2014）。  GGNN 滤波器还考虑了边缘类型和边缘方向。 为此，ei,j 表示从节点vi 到节点vj 的有向边，ei,j 的边类型为ti,j。  GGNN中基于循环的滤波器ffilter的传播过程可以总结如下：

![]()

其中 A ∈ Rdn×2dn 是一个矩阵，确定图中的节点如何相互通信。  n 是图中的节点数。  Ai：∈Rd×2d 是节点vi对应的A中的两列块。 在方程式中。  （14），初始节点特征 xi 用额外的零填充，使输入大小等于隐藏大小。 等式 (15) 计算 a(l) i ∈ R2d，方法是通过传入和传出边聚合来自不同节点的信息，参数取决于边类型和方向。 接下来的步骤使用 GRU 单元通过合并 a(l) i 和前一个时间步隐藏状态 h(l−1) i 来更新节点 v 的隐藏状态。

##### 3.2.2 图池化























