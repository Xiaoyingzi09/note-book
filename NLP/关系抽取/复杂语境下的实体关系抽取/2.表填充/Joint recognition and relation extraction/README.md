# Joint entity recognition and relation extraction as a multi-head selection problem

# 作为多头选择问题的联合实体识别和关系提取

## 摘要

用于联合实体识别和关系提取的最先进模型强烈依赖于外部自然语言处理 (NLP) 工具，例如 POS（词性）标注器和依赖解析器。 因此，这种联合模型的性能取决于从这些 NLP 工具获得的特征的质量。 但是，对于各种语言和上下文，这些功能并不总是准确的。 在本文中，我们提出了一种联合神经模型，它同时执行实体识别和关系提取，无需任何手动提取的特征或使用任何外部工具。 具体来说，我们使用 CRF（条件随机场）层对实体识别任务进行建模，并将关系提取任务建模为多头选择问题（即，可能为每个实体识别多个关系）。 我们提出了一个广泛的实验设置，以使用来自各种上下文（即新闻、生物医学、房地产）和语言（即英语、荷兰语）的数据集来证明我们的方法的有效性。 我们的模型优于之前使用自动提取特征的神经模型，同时它在基于特征的神经模型的合理范围内执行，甚至击败它们。

关键词：实体识别、关系抽取、多头选择、联合模型、序列标注

## 1 前言

实体识别和关系抽取的目标是从非结构化文本中发现实体提及的关系结构。 它是信息提取中的一个核心问题，因为它对于知识库填充和问答等任务至关重要。

该问题传统上作为两个独立的子任务来处理，即 (i) 命名实体识别 (NER) (Nadeau & Sekine, 2007) 和 (ii) 关系提取 (RE) (Bach & Badaskar, 2007)，在管道设置中。 管道模型的主要限制是：（i）组件（即 NER 和 RE）之间的错误传播和（ii）来自一项任务的可能有用信息未被另一项利用（例如，识别关系可能的 Works 有助于 NER 模块检测两个实体的类型，即 PER、ORG，反之亦然）。 另一方面，最近的研究建议使用联合模型来检测实体及其关系，以克服上述问题并实现最先进的性能（Li & Ji，2014；Miwa & Sasaki，2014）。

以前的联合模型严重依赖手工制作的功能。 神经网络的最新进展缓解了手动特征工程的问题，但其中一些仍然依赖于 NLP 工具（例如，词性标注器、依赖解析器）。  Miwa & Bansal (2016) 提出了一种基于循环神经网络 (RNN) 的联合模型，该模型使用双向顺序 LSTM（长短期记忆）对实体进行建模，并使用树-LSTM 将依赖树信息考虑在内来对关系进行建模 实体之间。 依赖信息是使用外部依赖解析器提取的。 同样，在 Li 等人的工作中。  (2017) 为了从生物医学文本中提取实体和关系，应用了一个也使用树-LSTM 的模型来提取依赖信息。 古普塔等人。  (2016) 提出了一种依赖 RNN 的方法，但使用了大量手工制作的特征和额外的 NLP 工具来提取特征，如 POS 标签等。 Adel & Schüutze (2017) 使用卷积复制实体周围的上下文 神经网络（CNN）。 请注意，上述工作检查实体对以进行关系提取，而不是直接对整个句子建模。 这意味着不会考虑同一句子中其他实体对的关系——这可能有助于决定特定对的关系类型。  Katiyar & Cardie (2017) 提出了一种基于 LSTM 的神经联合模型，他们一次对整个句子进行建模，但仍然没有处理多重关系的原则性方法。 贝库利斯等人。  (2018) 引入了二次评分层来同时对两个任务进行建模。 这种方法的局限性在于只能将单个关系分配给令牌，而与具有线性复杂度的标准方法相比，实体识别任务的时间复杂度有所增加。

在这项工作中，我们专注于一个新的通用联合模型，它同时执行实体识别和关系提取这两个任务，并且可以一起处理多个关系。 我们的模型在许多不同的上下文（即新闻、生物医学、房地产）和语言（即英语、荷兰语）中实现了最先进的性能，而无需依赖任何手动设计的功能或额外的 NLP 工具。 总之，我们提出的模型（将在第 3 节中详细介绍）解决了我们在相关工作（第 2 节）中确定的联合实体识别和关系提取的几个缺点：（i）我们的模型不依赖于外部 NLP 工具，也不依赖于 手工制作的特征，（ii）同时提取同一文本片段（通常是一个句子）中的实体和关系，其中（iii）一个实体可以同时涉及多个关系。

具体来说，Miwa & Bansal (2016) 的模型依赖于依赖解析器，它在特定语言（即英语）和上下文（即新闻）上表现特别好。 然而，我们的目标是开发一个在各种设置中都能很好地泛化的模型，因此只使用在训练期间学习的自动提取的特征。例如，Miwa & Bansal (2016) 和 Li 等人。  (2017) 在不同的上下文中使用完全相同的模型，即分别使用新闻 (ACE04) 和生物医学数据 (ADE)。 将我们的结果与 ADE 数据集进行比较，我们在 NER 任务上获得了 1.8% 的改进，在 RE 任务上获得了约 3% 的改进。 另一方面，我们的模型在 ACE04 数据集上的表现在合理的范围内（在 NER 任务中为 ~0.6%，在 RE 任务中为 ~1%），而没有使用预先计算的特征。 这表明 Miwa & Bansal (2016) 的模型强烈依赖于依赖解析器提取的特征，不能很好地泛化到依赖解析器特征较弱的不同上下文中。
   与 Adel & Schüutze (2017) 相比，我们通过一次对所有实体和句子的关系进行建模来训练我们的模型。 这种类型的推理有利于获取有关相邻实体和关系的信息，而不是每次只检查一对实体。 最后，我们解决了 Katiyar & Cardie (2017) 和 Bekoulis 等人提出的模型的潜在问题。  (2017)，他们基本上假设类（即关系）是互斥的：我们通过将关系提取组件表述为多标签预测问题来解决这个问题。 1

为了证明所提出方法的有效性，我们在联合执行实体识别和关系提取（参见第 4 节和第 5 节）方面进行了迄今为止最大的实验评估（据我们所知），使用来自各个领域的不同数据集（ 即新闻、生物医学、房地产）和语言（即英语、荷兰语）。 具体来说，我们将我们的方法应用于四个数据集，即 ACE04（新闻）、药物不良事件（ADE）、荷兰房地产分类（DREC）和 CoNLL'04（新闻）。 我们的方法优于所有不依赖任何附加特征或工具的最先进方法，而与利用手工工程特征或 NLP 工具的方法相比，其性能非常接近（甚至在生物医学数据集中更好）  .

## 2 相关工作

实体识别和关系提取的任务可以在管道设置（Fundel 等人，2007；Gurulingappa 等人，2012a；Bekoulis 等人，2017）或联合模型（Miwa 和 Sasaki）中一一应用 , 2014; Miwa & Bansal, 2016; Bekoulis 等人, 2018)。 在本节中，我们将介绍每个任务的相关工作（即命名实体识别和关系提取）以及联合实体和关系提取的先前工作。

#### 2.1 命名实体识别

在我们的工作中，NER 是我们解决的第一个任务，以解决端到端关系提取问题。 已经提出了许多基于手工特征的 NER 任务的不同方法，例如 CRF（Lafferty 等人，2001 年）、最大边际马尔可夫网络（Taskar 等人，2003 年）和支持向量机（  SVM）用于结构化输出（Tsochantaridis 等人，2004 年），仅举几例。 最近，基于 CNN 和 RNN 的模型等深度学习方法已与 CRF 损失函数（Collobert 等人，2011 年；Huang 等人，2015 年；Lample 等人，2016 年；Ma & Hovy，2016 年）相结合，用于 纳。 这些方法在不依赖手工制作的特征的情况下，在公开可用的 NER 数据集上实现了最先进的性能。

#### 2. 关系抽取

我们将关系提取视为联合模型的第二个任务。 关系提取的主要方法依赖于手工制作的特征（Zelenko 等人，2003 年；Kambhatla，2004 年）或神经网络（Socher 等人，2012 年；Zeng 等人，2014 年）。基于特征的方法侧重于获得有效的手工特征，例如定义核函数（Zelenko 等人，2003 年；Culotta & Sorensen，2004 年）和设计词汇、句法、语义特征等（Kambhatla，2004 年；Rink &  Harabagiu，2010 年）。 已经提出了神经网络模型来克服手动设计手工制作的特征从而提高性能的问题。  CNN-（Zeng 等，2014；Xu 等，2015a；dos Santos 等，2015）和基于 RNN（Socher 等，2013；Zhang & Wang，2015；Xu 等，2015b） 引入了模型来自动提取词汇和句子级别的特征，从而更深入地理解语言。 武等人。  (2016) 使用集成方案将 CNN 和 RNN 结合起来，以实现最先进的结果。

#### 2.3 联合实体和关系抽取

实体和关系提取包括以下任务：（i）识别实体（在 2.1 节中描述）和（ii）提取它们之间的关系（在 2.2 节中描述）。已经提出了基于特征的联合模型（Kate & Mooney，2010；Yang & Cardie，2013；Li & Ji，2014；Miwa & Sasaki，2014）同时解决实体识别和关系提取 (RE) 子任务。 这些方法依赖于 NLP 工具（例如，词性标注器）或手动设计的特征的可用性，因此 (i) 需要额外的数据预处理工作，(ii) 在 NLP 工具不可靠的不同应用程序和语言设置中表现不佳 , (iii) 增加计算复杂度。 在本文中，我们引入了一种联合神经网络模型来克服上述问题并自动执行端到端关系提取，而无需任何手动特征工程或使用额外的 NLP 组件。

神经网络方法已被考虑用于解决联合设置（端到端关系提取）中的问题，通常包括使用 RNN 和 CNN（Miwa 和 Bansal，2016 年；Zheng 等人，2017 年；Li 等人，2016 年）。  , 2017)。 具体而言，Miwa & Bansal (2016) 提出使用双向树结构 RNN 来捕获依赖树信息（其中使用最先进的依赖解析器提取解析树），这已被证明对关系提取有益（Xu et  al., 2015a,b)。 李等人。  （2017 年）将 Miwa 和 Bansal（2016 年）的工作应用于生物医学文本，报告了两个生物医学数据集的最新性能。 古普塔等人。(2016) 建议使用大量手工制作的特征以及 RNN。  Adel & Schüutze (2017) 解决了实体分类任务（与 NER 不同，因为在实体分类中实体的边界是已知的，并且只应该预测实体的类型）和使用近似值的关系提取问题 全局归一化目标（即 CRF）：他们复制句子的上下文（实体的左右部分），一次将一个实体对馈送到 CNN 以进行关系提取。 因此，它们不会同时推断同一句子中的其他潜在实体和关系。  Katiyar & Cardie (2017) 和 Bekoulis 等人。  (2018) 在不使用任何依赖解析树特征的情况下，研究了带有注意力的 RNN，用于提取实体提及之间的关系。 与 Katiyar & Cardie (2017) 不同，在这项工作中，我们通过使用 sigmoid 损失获得多重关系和 NER 组件的 CRF 损失，将问题框架化为多头选择问题。 通过这种方式，我们能够独立预测不相互排斥的类，而不是在标记之间分配相等的概率值。

![]()

我们克服了 Bekoulis 等人描述的额外复杂性问题。  (2018)，通过将损失函数划分为 NER 和关系提取组件。 此外，我们能够处理多个关系，而不仅仅是预测单个关系，如 Bekoulis 等人对结构化房地产广告的应用所描述的那样。  (2018)。

## 3 Joint model

在本节中，我们展示了图 1 所示的多头联合模型。该模型能够同时识别实体（即类型和边界）以及它们之间的所有可能关系。 我们将问题表述为扩展先前工作（Zhang 等人，2017 年；Bekoulis 等人，2018 年）的多头选择问题，如第 2.3 节所述。 通过多头，我们的意思是任何特定实体都可能涉及与其他实体的多重关系。 如图 1 所示，模型的基本层是：（i）嵌入层，（ii）双向顺序 LSTM（BiLSTM）层，（iii）CRF 层和（iv）sigmoid 评分层。 在图 1 中，展示了来自 CoNLL04 数据集的一个例句。 我们模型的输入是一系列标记（即句子的单词），然后将其表示为词向量（即词嵌入）。  BiLSTM 层能够通过 RNN 结构为包含上下文的每个单词提取更复杂的表示。 然后 CRF 和 sigmoid 层能够为这两个任务产生输出。 每个令牌（例如，Smith）的输出是双重的：（i）实体识别标签（例如，I-PER，表示令牌位于 PER 类型的命名实体内）和（ii）包含头部的一组元组 实体的标记以及它们之间的关系类型（例如，{(Center, Works for), (Atlanta, Lives in)}）。 由于我们假设基于令牌的编码，我们只考虑实体的最后一个令牌作为另一个令牌的头部，消除冗余关系。例如，实体“John Smith”和“Disease Control Center”之间存在关系的Works。 我们没有连接实体的所有代币，而是仅将“Smith”与“Center”连接起来。 此外，对于没有关系的情况，我们引入了“N”标签，我们将令牌本身预测为头部。

![]()

#### 3.1 嵌入层

给定一个句子 w = w1, ..., wn 作为一个标记序列，词嵌入层负责将每个标记映射到一个词向量（wword2vec）。 我们使用 Skip-Gram word2vec 模型 (Mikolov et al., 2013) 使用预先训练的词嵌入。

在这项工作中，我们还使用了字符嵌入，因为它们通常应用于神经 NER（Ma & Hovy，2016 年；Lample 等人，2016 年）。 这种类型的嵌入能够捕获形态特征，例如前缀和后缀。 例如，在药物不良事件 (ADE) 数据集中，后缀“毒性”可以指定药物不良事件实体，例如“神经毒性”或“肝毒性”，因此信息量很大。 另一个例子可能是荷兰房地产分类 (DREC) 数据集中的荷兰语后缀“kamer”（英语中的“room”），用于指定空间实体“badkamer”（英语中的“浴室”）和“slaapkamer”（ 英语中的“卧室”）。在训练期间学习字符级嵌入，类似于 Ma & Hovy (2016) 和 Lample 等人。  (2016)。 在兰普尔等人的工作中。  (2016)，就 NER F1 分数而言，字符嵌入使性能提高了 3%。 在我们的工作中，通过结合字符嵌入，我们在表 2 中报告了总体 F1 得分增加了约 2%。 有关更多详细信息，请参阅第 5.2 节。

图 2 说明了基于其字符生成词嵌入的神经架构。 每个单词的字符由字符向量（即嵌入）表示。 字符嵌入被馈送到 BiLSTM 并连接两个最终状态（向前和向后）。 向量 wchars 是单词的字符级表示。 然后将该向量进一步连接到词级表示 wword2vec 以获得完整的词嵌入向量。

#### 3.2 双向LSTM编码层

RNN 通常用于对序列数据进行建模，并已成功应用于各种 NLP 任务（Sutskever 等人，2014 年；Lample 等人，2016 年；Miwa 和 Bansal，2016 年）。 在这项工作中，我们使用多层 LSTM，这是一种特定类型的 RNN，能够很好地捕获长期依赖关系（Bengio 等人，1994 年；Pascanu 等人，2013 年）。 我们采用了一个 BiLSTM，它能够从左到右（过去到未来）和从右到左（未来到过去）对信息进行编码。 这样，我们可以通过在时间步 i 连接前向 (⃗hi) 和后向 (⃗ hi) 输出来组合每个单词的双向信息。 时间步 i 处的 BiLSTM 输出可以写为：

![]()

#### 3.3 命名实体识别

我们将实体识别任务制定为序列标记问题，类似于之前在联合学习模型（Miwa & Bansal，2016；Li 等人，2017 年；Katiyar & Cardie，2017 年）和命名实体识别（Lample 等人，  2016 年；Ma & Hovy，2016 年）使用 BIO（开始、内部、外部）编码方案。 每个实体由句子中的多个连续标记组成，我们应该为句子中的每个标记分配一个标签。 这样我们就能够识别实体参数（开始和结束位置）及其类型（例如，ORG）。 为此，我们将 B 类型（开始）分配给实体的第一个标记，将 I 类型（内部）分配给实体内的所有其他标记，如果标记不属于某个标记，则将 O 标记（外部）分配给实体。 实体。 图 1 显示了分配给句子标记的 BIO 编码标签的示例。 在 CRF 层，可以观察到我们分配 B-ORG 和 I-ORG 标签分别指示实体“疾病控制中心”的开始和内部标记。 在 BiLSTM 层之上，我们使用 softmax 或 CRF 层来计算每个标记的最可能的实体标签。 我们计算每个实体标签的每个 token wi 的分数：

![]()

其中上标 (e) 用于表示 NER 任务，f(·) 是元素激活函数（即 relu，tanh），V (e) ∈ Rp×l，U (e) ∈ Rl×2d  , b(e) ∈ Rl，d 是 LSTM 的隐藏大小，p 是 NER 标签（例如，B-ORG）的数量，l 是层的宽度。 我们计算给定标记 wi 的所有候选标签的概率为 Pr(tag | wi) = softmax(s(hi)) 其中 Pr(tag | wi) ∈ Rp。 在这项工作中，我们仅将 softmax 方法用于实体分类 (EC) 任务（类似于 NER），其中假设边界已给定，我们只需要预测每个标记的实体类型（例如，PER）。  CRF 方法用于 NER 任务，包括实体类型和边界识别。

![]()

在 softmax 方法中，我们在预测时以贪婪的方式将实体类型分配给令牌（即，所选标签只是所有可能标签集中得分最高的标签）。 尽管假设独立的标签分布有利于实体分类任务（例如，词性标注），但当标签之间存在强依赖性时，情况并非如此。 具体来说，在 NER 中，BIO 标记方案强制执行了几个限制（例如，B-LOC 不能跟在 I-PER 之后）。 即使 BiLSTM 捕获了有关相邻单词的信息，softmax 方法也允许进行局部决策（即，对于每个标记 wi 的标签）。 尽管如此，对于特定令牌的标签决策，不考虑相邻标签。 例如，在实体“John Smith”中，将“Smith”标记为 PER 有助于确定“John”是 B-PER。 为此，对于 NER，我们使用线性链 CRF，类似于 Lample 等人。  （2016 年），其中报告了使用 CRF 时约 1% F1 NER 点的改进。 在我们的例子中，通过使用 CRF，我们还报告了 1% 的整体性能改进，如表 2 所示（参见第 5.2 节）。 假设词向量 w、得分向量序列 s(e) 1 , ..., s(e) n 和标签预测向量 y(e) 1 , ..., y(e) n ，线性 -chain CRF 分数定义为：

![]()

我们应用 Viterbi 来获得得分最高的标签序列 ˆy(e)。 我们通过最小化交叉熵损失 LNER 来训练 softmax（用于 EC 任务）和 CRF 层（用于 NER）。 我们还通过学习标签嵌入使用实体标签作为我们关系提取层的输入，这是由 Miwa & Bansal (2016) 推动的，其中报告了 2% F1 的改进（使用标签嵌入）。 在我们的例子中，标签嵌入导致 F1 分数增加 1%，如表 2 所示（参见第 5.2 节）。下一层的输入是双重的：LSTM 的输出状态和学习的标签嵌入表示，编码了命名实体知识可用于关系提取的直觉。 在训练期间，我们使用黄金实体标签，而在预测时，我们使用预测的实体标签作为下一层的输入。 下一层的输入是隐藏 LSTM 状态 hi 与标记 wi 的标签嵌入 gi 的串联：

![]()

#### 3.4 关系提取作为多头选择

在本小节中，我们描述了关系提取任务，该任务被表述为一个多头选择问题（Zhang 等人，2017 年；Bekoulis 等人，2018 年）。 在我们方法的一般表述中，每个标记 wi 可以有多个头（即与其他标记的多个关系）。 我们预测元组 (ˆyi, ˆci)，其中 ˆyi 是正面的向量，而 ˆci 是每个标记 wi 的对应关系的向量。 这与之前的依赖解析方法（Zhang et al., 2017）的标准头部选择不同，因为（i）它被扩展到预测多个头部并且（ii）头部和关系的决定是联合采取的（即， 而不是首先预测头部，然后在下一步中通过使用额外的分类器来预测关系）。 给定一个标记序列 w 和一组关系标签 R 作为输入，我们的目标是为每个标记 wi 识别，i ∈ {0, ..., n} 最可能的头的向量 ˆyi ⊆ w 和 最可能的对应关系标签 ˆri ⊆ R。我们计算标记 wi 和 wj 之间的分数给定标签 rk 如下：

![]()

其中上标 (r) 用于关系任务的表示法，f(·) 是元素激活函数（即 relu，tanh），V (r) ∈ Rl，U (r) ∈ Rl×(2d+  b), W (r) ∈ Rl×(2d+b), b(r) ∈ Rl, d 是 LSTM 的隐藏大小，b 是标签嵌入的大小，l 是层宽。 我们定义

![]()

是标记 wj 被选为标记 wi 的头部的概率，它们之间的关系标签为 rk，其中 σ(.) 代表 sigmoid 函数。 我们在训练期间最小化交叉熵损失 Lrel：

![]()

其中 yi ⊆ w 和 ri ⊆ R 是头部的真实向量和 wi 的关联关系标签，m 是 wi 的关系（头部）的数量。 训练后，我们保持头 yi 和关系标签 ri 的组合超过基于等式中定义的估计联合概率的阈值。  (7). 与之前关于联合模型的工作 (Katiyar & Cardie, 2017) 不同，我们能够预测多个关系，将类视为独立而不是相互排斥的（不同类的概率总和不一定为 1）。 对于联合实体和关系提取任务，我们将最终目标计算为 LNER + Lrel。

#### 3.5 埃德蒙兹算法

我们的模型能够同时提取实体提及和它们之间的关系。 为了证明我们模型的有效性和通用性，我们还在最近推出的荷兰房地产分类 (DREC) 数据集（Bekoulis 等人，2017 年）上对其进行了测试，其中实体需要形成树状结构。 通过使用阈值推理，不能保证关系的树结构。 因此，我们应该对我们的模型实施树结构约束。 为此，我们使用 Edmonds 的有向图最大生成树算法 (Chu & Liu, 1965; Edmonds, 1967) 对系统的输出进行后处理。 构建了一个完全连接的有向图 G = (V, E)，其中顶点 V 表示已识别实体的最后一个标记（由 NER 预测），边 E 表示以其得分作为权重的最高得分关系。  Edmonds 算法适用于尚未通过阈值推理形成树的情况。

## 4 实验











































