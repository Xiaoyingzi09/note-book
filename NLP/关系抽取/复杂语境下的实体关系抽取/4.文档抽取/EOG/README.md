# Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs

# 关联点：使用面向边缘的图进行文档级神经关系提取

## 摘要

文档级关系提取是一个复杂的人工过程，需要逻辑推理来提取文本中命名实体之间的关系。 现有方法使用基于图的神经模型，将单词作为节点，边作为它们之间的关系，对句子之间的关系进行编码。 这些模型是基于节点的，即它们仅基于两个目标节点表示形成对表示。 然而，实体关系可以通过形成为节点之间的路径的唯一边表示来更好地表达。 因此，我们提出了一种用于文档级关系提取的面向边缘的图神经模型。该模型利用不同类型的节点和边来创建文档级图。 图边上的推理机制能够在内部使用多实例学习来学习句内和句间关系。在用于化学疾病和基因疾病关联的两个文档级生物医学数据集上的实验表明，所提出的面向边缘的方法是有用的。

## 1 介绍

文本中命名实体之间关系的提取，称为关系提取（RE），是自然语言处理（NLP）的一项重要任务。 最近，为了提高当前方法的推理能力，RE 引起了该领域的广泛关注（Zeng 等，2017；Christopoulou 等，2018；Luan 等，2019）。

![]()

图 1：改编自 CDR 数据集的文档级句间关系示例（Li 等人，2016a）。 实线和虚线分别代表句内和句间关系。

在现实世界的场景中，大量的关系是跨句子表达的。 识别这些关系的任务称为句间 RE。 通常，句间关系出现在包含多个句子的文本片段中，例如文档。 在这些片段中，每个实体通常用相同的短语或别名重复，这些短语或别名的出现通常被命名为实体提及并被视为实体的实例。 目标实体在不同句子中的多次提及可用于识别句间关系，因为这些关系可能取决于它们与同一文档中其他实体的提及的交互。

如图 1 的示例所示，实体双侧视神经病变、乙胺丁醇和异烟肼各有两个提及，而实体暗点有一个提及。 化学乙胺丁醇与疾病暗点之间的关系显然是句间关系。 只有当我们考虑不同句子中这些实体的提及之间的相互作用时，才能确定它们的关联。 在第一句话中提到双侧视神经病变与提到乙胺丁醇相互作用。 另一个提到前者与第三句中提到的暗点相互作用。 这个交互链可以帮助我们推断实体乙胺丁醇与实体暗点有关系。

目前 arXiv:1909.00228v1 [cs.CL] 2019 年 8 月 31 日用于处理多次提及命名实体的最常用技术是多实例学习 (MIL)。 最初，MIL 是由 Riedel 等人引入的。  (2010) 以减少远程监督语料库中的噪声 (Mintz et al., 2009)。 在 DS 中，训练实例是使用知识库 (KB) 实体链接和带有启发式规则的自动注释从大型原始语料库创建的。 此设置中的 MIL 考虑包含一对实体的多个句子（包）作为该对的多个实例。维尔加等人。  (2018) 引入了另一种 MIL 设置，用于文档中命名实体之间的关系提取。 在此设置中，映射到相同 KB ID 的实体被视为实体概念的提及，并且提及对对应于该对的多个实例。然而，文档级 RE 在一般领域并不常见，因为感兴趣的实体类型通常可以在同一个句子中找到（Banko 等，2007）。 相反，在生物医学领域，鉴于生物医学实体可以拥有众多别名，文档级关系尤为重要（Quirk 和 Poon，2017 年）。

为了处理文档级 RE，最近的方法假设文档中只提到了两个目标实体（Nguyen 和 Verspoor，2018 年；Verga 等人，2018 年）或使用不同的模型进行句内和句间 RE（  Gu 等人，2016；Li 等人，2016b；Gu 等人，2017）。 与采用顺序模型的方法相比（Nguyen 和 Verspoor，2018 年；Gu 等人，2017 年；Zhou 等人，2016 年），基于图的神经方法已被证明在编码长距离、句间信息方面很有用（Peng 等人，2016 年）  .，2017；Quirk 和 Poon，2017；Gupta 等，2019）。这些模型将单词解释为节点，将它们之间的连接解释为边。 它们通常通过在训练期间更新表示来在节点上执行。 但是，两个实体之间的关系取决于不同的上下文。因此，它可以用对唯一的边连接更好地表达。 解决这个问题的一个直接方法是创建基于图的模型，这些模型依赖于边表示，而不是专注于节点表示，节点表示在多个实体对之间共享。

在这项工作中，我们使用 MIL 和基于图的神经模型来处理文档级、句内和句间 RE。 我们的目标是通过利用文档中的其他交互来推断两个实体之间的关系。 我们构建了一个具有异构类型节点和边的文档图，以更好地捕获节点之间的不同依赖关系。 在提议的图中，节点对应于实体、提及或句子，而不是单词。 我们基于简单的启发式规则连接不同的节点，并为连接的节点生成不同的边表示。 为了实现我们的目标，我们将模型设计为面向边的，因为它学习边表示（在图节点之间）而不是节点表示。 图边上的迭代算法用于以边表示的形式对节点之间的依赖性进行建模。 通过使用这些边来预测句内和句间实体关系。 我们的贡献可以总结如下：

- 我们提出了一种新颖的面向边缘的图神经模型，用于文档级关系提取。
     该模型不同于现有的图模型，因为它专注于构建独特的节点和边，将信息编码为边表示而不是节点表示。
- 提议的模型独立于句法依赖工具，并且可以在手动注释的文档级化学疾病交互数据集上实现最先进的性能。
- 对模型组件的分析表明文档级图可以有效地编码文档级依赖性。
     此外，我们表明句间关联可以有利于检测句内关系。

## 2 建议的模型

我们将我们的模型构建为我们之前提出的用于文档级 RE 的句子级模型（Christopoulou 等人，2018 年）的重要扩展。 两种模型最关键的区别是引入和构建了部分连接的文档图，而不是完全连接的句子级图。此外，与仅包含实体节点和单个边类型的句子级图相比，文档图由异构类型的节点和边组成。 此外，当提及级别的注释可用时，所提出的方法利用多实例学习。、

![]()

图 2：所提出方法的抽象架构。 该模型接收一个文档并分别对每个句子进行编码。 构建文档级图并将其输入迭代算法以生成目标实体节点之间的边表示。 为简洁起见，未显示某些节点连接。

如图 2 所示，所提出的模型由四层组成：句子编码层、图构建层、推理层和分类层。该模型接收包含已识别概念级实体及其文本提及的文档作为输入。 接下来，构建具有多种类型的节点和边的文档级图。 将推理算法应用于图边以生成概念级对表示。 在最后一层，目标概念实体节点之间的边表示被分类为关系类别。

对于本节的其余部分，我们首先简要介绍文档级 RE 任务设置，然后解释所提出模型的四个层。

#### 2.1 任务设置

在概念上，文档级 RE 输入被视为带注释的文档。 注释包括概念级实体（具有分配的 KB ID），以及每个实体在同一个别名短语下的多次出现，即实体提及。 我们考虑提及与给定概念实体的关联（也称为实体链接（Shen et al., 2014））。 任务的目标是给定一个带注释的文档，以识别该文档中所有相关的概念级别对。 在这项工作中，我们将概念级注释称为实体，将提及级注释称为提及。

#### 2.2 句子编码层

首先，输入文档的句子中的每个词都被转换成一个密集的向量表示，即词嵌入。 然后将每个句子的向量化单词输入到双向 LSTM 网络 (BiLSTM)（Hochreiter 和 Schmidhuber，1997；Schuster 和 Paliwal，1997），命名为编码器。 编码器的输出为输入句子的每个词产生上下文化的表示。

#### 2.3 图结构层

来自编码器的上下文词表示用于构建文档级图形结构。 图层包括两个子层，节点构建层和边构建层。 我们组合了第一个子层中图节点的表示和第二个子层中边的表示。

##### 2.3.1 节点结构

我们在图中形成了三种不同类型的节点：提及节点 (M) nm、实体节点 (E) ne 和句子节点 (S) ns。 每个节点表示被计算为不同元素嵌入的平均值。 首先，提及节点对应于输入文档中实体的不同提及。 提及节点的表示形成为提及所包含的词 (w) 的平均值。 其次，实体节点代表独特的实体概念。 实体节点的表示被计算为与实体关联的提及 (m) 表示的平均值。 最后，句子节点对应于句子。一个句子节点表示为句子中单词表示的平均值。 为了区分图中不同的节点类型，我们将节点类型 (t) 嵌入到每个节点表示中。然后将最终节点表示估计为 nm = [avgwi∈m(wi);  tm], ne = [avgmi∈e(mi);  te], ns = [avgwi∈s(wi);  ts]。

#### 2.3.2 边结构

我们最初使用启发式规则构建图节点之间的无向边，这些规则源于文档元素（即提及、实体和句子）之间的自然关联。 由于我们无法提前知道两个实体是否相关，因此我们不直接连接实体节点。 节点之间的连接基于预定义的文档级交互。 该模型的目标是使用图中的其他现有边生成实体到实体 (EE) 边表示，从而推断实体到实体的关系。 下面描述了不同的预定义边缘类型。

提及（MM）：句子中提及的同时出现可能是交互的弱指示。 出于这个原因，只有当相应的提及位于同一个句子中时，我们才会创建提及到提及的边缘。 每个提及对 mi 和 mj 之间的边表示是通过连接节点的表示、上下文 cmi,mj 和与两个提及之间的距离 dmi,mj 相关的距离嵌入来生成的，就中间词而言：xMM = [ 纳米； 纳米；  cmi,mj;  dmi，mj]。在这里，我们为这些对生成上下文表示，以便对本地的、以对为中心的信息进行编码。 我们使用基于参数的注意力机制（Wang 等人，2016）来衡量句子中其他单词对提及的重要性，将 k ∈ {1, 2} 表示为提及参数。

![]()

其中 nmk 是提及节点表示，wi 是句子词表示，ai 是提及对 m1、m2 的词 i 的注意力权重，H ∈ Rw×d 是句子词表示矩阵，a ∈ Rw 是注意力权重向量 对于该对，cm1,m2 是提及对的最终上下文表示。

Mention-Sentence (MS)：仅当提及位于句子中时，才会连接提及到句子的节点。 它们的初始边缘表示被构造为提及节点和句子节点的串联，xMS = [nm;  ns]。

提及实体（ME）：如果提及与实体相关联，我们将提及节点连接到实体节点，xME = [nm;  ne]。

Sentence-Sentence (SS)：受 Quirk 和 Poon (2017) 的启发，我们连接句子节点以编码非本地信息。 与先前工作的主要区别在于我们的边缘是未标记的、无方向的并且跨越多个句子。 为了编码句子之间的距离，我们以嵌入的形式连接到句子节点表示它们的距离：xSS = [nsi;  nsj;  dsi,sj]。 我们连接图中的所有句子节点。 我们分别将 SSdirect 视为直接、有序的边（距离等于 1），将 SSindirect 视为 S 节点之间的间接、无序边（距离 > 1）。 在我们的设置中，SS 表示 SSdirect 和 SSindirect 的组合。

实体句（ES）：为了直接对实体句关联建模，我们将实体节点连接到句子节点，如果该实体至少有一个提及位于该句子中，xES = [ne;  ns]。

为了产生等维的边缘表示，我们对不同的边缘表示使用不同的线性缩减层，

![]()

其中 e(1) z 是长度为 1 的边表示，Wz ∈ Rdz×d 对应于学习矩阵，z ∈ [MM, MS, ME, SS, ES]。

#### 2.4 推理层

我们利用迭代算法在图中不同节点之间生成边，以及更新现有边。 我们仅使用第 2.3.2 节中描述的边初始化图，这意味着不存在直接的实体到实体 (EE) 边。 我们只能通过表示节点之间的路径来生成 EE 边表示。这意味着实体可以通过最小长度等于 32 的边路径关联。

为此，我们调整了 Christopoulou 等人提出的两步推理机制。(2018)，对图中节点和边之间的交互进行编码，从而对 EE 关联进行建模。

第一步，我们的目标是使用中间节点 k 在两个节点 i 和 j 之间生成一条路径。 因此，我们使用修改后的双线性变换组合了两个连续边 eik 和 ekj 的表示。 此操作生成双倍长度的边表示。 我们将 i 和 j 到 k 之间的所有现有路径组合起来。  i、j 和 k 节点可以是 E、M 或 S 三种节点类型中的任何一种。与目标节点没有相邻边的中间节点将被忽略。

![]()

其中σ是sigmoid非线性函数，W∈Rdz×dz是学习参数矩阵，⊙指元素相乘，l是边的长度，eik对应节点i和k之间边的表示 .

在第二步中，我们使用线性插值将原始（短）边缘表示和由方程（3）产生的新（较长）边缘表示聚合如下：

![]()

其中 β ∈ [0, 1] 是一个标量，用于控制较短边缘呈现的贡献。 一般来说，较短边的 β 较大，因为我们期望通过它们之间的最短路径更好地表达两个节点之间的关系（Xu 等人，2015 年；Borgwardt 和 Kriegel，2005 年）。

这两个步骤重复有限次数 N。迭代次数与边缘表示的最终长度相关。初始边长 l 等于 1，第一次迭代的结果是长度为 2 的边。第二次迭代的结果是长度为 4 的边。类似地，在 N 次迭代后，边的长度将是 2N。

#### 2.5 分类层

为了对感兴趣的概念级实体对进行分类，我们结合了一个 softmax 分类器，使用对应于概念级实体对的文档图的实体到实体边 (EE)。

![]()

其中 Wc ∈ Rr×dz 和 bc ∈ Rr 是分类层的学习参数，r 是关系类别的数量。

## 3 实验设置

该模型是使用 PyTorch 开发的（Paszke 等，2017）。我们结合了早期停止来确定最佳训练时期，并使用 Adam（Kingma 和 Ba，2015）作为模型优化器。





















