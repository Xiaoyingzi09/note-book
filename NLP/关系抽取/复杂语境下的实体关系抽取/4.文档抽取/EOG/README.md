# Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs

# 关联点：使用面向边缘的图进行文档级神经关系提取

## 摘要

文档级关系提取是一个复杂的人工过程，需要逻辑推理来提取文本中命名实体之间的关系。 现有方法使用基于图的神经模型，将单词作为节点，边作为它们之间的关系，对句子之间的关系进行编码。 这些模型是基于节点的，即它们仅基于两个目标节点表示形成对表示。 然而，实体关系可以通过形成为节点之间的路径的唯一边表示来更好地表达。 因此，我们提出了一种用于文档级关系提取的面向边缘的图神经模型。该模型利用不同类型的节点和边来创建文档级图。 图边上的推理机制能够在内部使用多实例学习来学习句内和句间关系。在用于化学疾病和基因疾病关联的两个文档级生物医学数据集上的实验表明，所提出的面向边缘的方法是有用的。

## 1 介绍

文本中命名实体之间关系的提取，称为关系提取（RE），是自然语言处理（NLP）的一项重要任务。 最近，为了提高当前方法的推理能力，RE 引起了该领域的广泛关注（Zeng 等，2017；Christopoulou 等，2018；Luan 等，2019）。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/%E5%A4%8D%E6%9D%82%E8%AF%AD%E5%A2%83%E4%B8%8B%E7%9A%84%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/4.%E6%96%87%E6%A1%A3%E6%8A%BD%E5%8F%96/EOG/figure/figure_1.png)

图 1：改编自 CDR 数据集的文档级句间关系示例（Li 等人，2016a）。 实线和虚线分别代表句内和句间关系。

在现实世界的场景中，大量的关系是跨句子表达的。 识别这些关系的任务称为句间 RE。 通常，句间关系出现在包含多个句子的文本片段中，例如文档。 在这些片段中，每个实体通常用相同的短语或别名重复，这些短语或别名的出现通常被命名为实体提及并被视为实体的实例。 目标实体在不同句子中的多次提及可用于识别句间关系，因为这些关系可能取决于它们与同一文档中其他实体的提及的交互。

如图 1 的示例所示，实体双侧视神经病变、乙胺丁醇和异烟肼各有两个提及，而实体暗点有一个提及。 化学乙胺丁醇与疾病暗点之间的关系显然是句间关系。 只有当我们考虑不同句子中这些实体的提及之间的相互作用时，才能确定它们的关联。 在第一句话中提到双侧视神经病变与提到乙胺丁醇相互作用。 另一个提到前者与第三句中提到的暗点相互作用。 这个交互链可以帮助我们推断实体乙胺丁醇与实体暗点有关系。

目前 arXiv:1909.00228v1 [cs.CL] 2019 年 8 月 31 日用于处理多次提及命名实体的最常用技术是多实例学习 (MIL)。 最初，MIL 是由 Riedel 等人引入的。  (2010) 以减少远程监督语料库中的噪声 (Mintz et al., 2009)。 在 DS 中，训练实例是使用知识库 (KB) 实体链接和带有启发式规则的自动注释从大型原始语料库创建的。 此设置中的 MIL 考虑包含一对实体的多个句子（包）作为该对的多个实例。维尔加等人。  (2018) 引入了另一种 MIL 设置，用于文档中命名实体之间的关系提取。 在此设置中，映射到相同 KB ID 的实体被视为实体概念的提及，并且提及对对应于该对的多个实例。然而，文档级 RE 在一般领域并不常见，因为感兴趣的实体类型通常可以在同一个句子中找到（Banko 等，2007）。 相反，在生物医学领域，鉴于生物医学实体可以拥有众多别名，文档级关系尤为重要（Quirk 和 Poon，2017 年）。

为了处理文档级 RE，最近的方法假设文档中只提到了两个目标实体（Nguyen 和 Verspoor，2018 年；Verga 等人，2018 年）或使用不同的模型进行句内和句间 RE（  Gu 等人，2016；Li 等人，2016b；Gu 等人，2017）。 与采用顺序模型的方法相比（Nguyen 和 Verspoor，2018 年；Gu 等人，2017 年；Zhou 等人，2016 年），基于图的神经方法已被证明在编码长距离、句间信息方面很有用（Peng 等人，2016 年）  .，2017；Quirk 和 Poon，2017；Gupta 等，2019）。这些模型将单词解释为节点，将它们之间的连接解释为边。 它们通常通过在训练期间更新表示来在节点上执行。 但是，两个实体之间的关系取决于不同的上下文。因此，它可以用对唯一的边连接更好地表达。 解决这个问题的一个直接方法是创建基于图的模型，这些模型依赖于边表示，而不是专注于节点表示，节点表示在多个实体对之间共享。

在这项工作中，我们使用 MIL 和基于图的神经模型来处理文档级、句内和句间 RE。 我们的目标是通过利用文档中的其他交互来推断两个实体之间的关系。 我们构建了一个具有异构类型节点和边的文档图，以更好地捕获节点之间的不同依赖关系。 在提议的图中，节点对应于实体、提及或句子，而不是单词。 我们基于简单的启发式规则连接不同的节点，并为连接的节点生成不同的边表示。 为了实现我们的目标，我们将模型设计为面向边的，因为它学习边表示（在图节点之间）而不是节点表示。 图边上的迭代算法用于以边表示的形式对节点之间的依赖性进行建模。 通过使用这些边来预测句内和句间实体关系。 我们的贡献可以总结如下：

- 我们提出了一种新颖的面向边缘的图神经模型，用于文档级关系提取。
     该模型不同于现有的图模型，因为它专注于构建独特的节点和边，将信息编码为边表示而不是节点表示。
- 提议的模型独立于句法依赖工具，并且可以在手动注释的文档级化学疾病交互数据集上实现最先进的性能。
- 对模型组件的分析表明文档级图可以有效地编码文档级依赖性。
     此外，我们表明句间关联可以有利于检测句内关系。

## 2 建议的模型

我们将我们的模型构建为我们之前提出的用于文档级 RE 的句子级模型（Christopoulou 等人，2018 年）的重要扩展。 两种模型最关键的区别是引入和构建了部分连接的文档图，而不是完全连接的句子级图。此外，与仅包含实体节点和单个边类型的句子级图相比，文档图由异构类型的节点和边组成。 此外，当提及级别的注释可用时，所提出的方法利用多实例学习。、

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/%E5%A4%8D%E6%9D%82%E8%AF%AD%E5%A2%83%E4%B8%8B%E7%9A%84%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/4.%E6%96%87%E6%A1%A3%E6%8A%BD%E5%8F%96/EOG/figure/figure_2.png)

图 2：所提出方法的抽象架构。 该模型接收一个文档并分别对每个句子进行编码。 构建文档级图并将其输入迭代算法以生成目标实体节点之间的边表示。 为简洁起见，未显示某些节点连接。

如图 2 所示，所提出的模型由四层组成：句子编码层、图构建层、推理层和分类层。该模型接收包含已识别概念级实体及其文本提及的文档作为输入。 接下来，构建具有多种类型的节点和边的文档级图。 将推理算法应用于图边以生成概念级对表示。 在最后一层，目标概念实体节点之间的边表示被分类为关系类别。

对于本节的其余部分，我们首先简要介绍文档级 RE 任务设置，然后解释所提出模型的四个层。

#### 2.1 任务设置

在概念上，文档级 RE 输入被视为带注释的文档。 注释包括概念级实体（具有分配的 KB ID），以及每个实体在同一个别名短语下的多次出现，即实体提及。 我们考虑提及与给定概念实体的关联（也称为实体链接（Shen et al., 2014））。 任务的目标是给定一个带注释的文档，以识别该文档中所有相关的概念级别对。 在这项工作中，我们将概念级注释称为实体，将提及级注释称为提及。

#### 2.2 句子编码层

首先，输入文档的句子中的每个词都被转换成一个密集的向量表示，即词嵌入。 然后将每个句子的向量化单词输入到双向 LSTM 网络 (BiLSTM)（Hochreiter 和 Schmidhuber，1997；Schuster 和 Paliwal，1997），命名为编码器。 编码器的输出为输入句子的每个词产生上下文化的表示。

#### 2.3 图结构层

来自编码器的上下文词表示用于构建文档级图形结构。 图层包括两个子层，节点构建层和边构建层。 我们组合了第一个子层中图节点的表示和第二个子层中边的表示。

##### 2.3.1 节点结构

我们在图中形成了三种不同类型的节点：提及节点 (M) nm、实体节点 (E) ne 和句子节点 (S) ns。 每个节点表示被计算为不同元素嵌入的平均值。 首先，提及节点对应于输入文档中实体的不同提及。 提及节点的表示形成为提及所包含的词 (w) 的平均值。 其次，实体节点代表独特的实体概念。 实体节点的表示被计算为与实体关联的提及 (m) 表示的平均值。 最后，句子节点对应于句子。一个句子节点表示为句子中单词表示的平均值。 为了区分图中不同的节点类型，我们将节点类型 (t) 嵌入到每个节点表示中。然后将最终节点表示估计为 nm = [avgwi∈m(wi);  tm], ne = [avgmi∈e(mi);  te], ns = [avgwi∈s(wi);  ts]。

#### 2.3.2 边结构

我们最初使用启发式规则构建图节点之间的无向边，这些规则源于文档元素（即提及、实体和句子）之间的自然关联。 由于我们无法提前知道两个实体是否相关，因此我们不直接连接实体节点。 节点之间的连接基于预定义的文档级交互。 该模型的目标是使用图中的其他现有边生成实体到实体 (EE) 边表示，从而推断实体到实体的关系。 下面描述了不同的预定义边缘类型。

提及（MM）：句子中提及的同时出现可能是交互的弱指示。 出于这个原因，只有当相应的提及位于同一个句子中时，我们才会创建提及到提及的边缘。 每个提及对 mi 和 mj 之间的边表示是通过连接节点的表示、上下文 cmi,mj 和与两个提及之间的距离 dmi,mj 相关的距离嵌入来生成的，就中间词而言：xMM = [ 纳米； 纳米；  cmi,mj;  dmi，mj]。在这里，我们为这些对生成上下文表示，以便对本地的、以对为中心的信息进行编码。 我们使用基于参数的注意力机制（Wang 等人，2016）来衡量句子中其他单词对提及的重要性，将 k ∈ {1, 2} 表示为提及参数。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/%E5%A4%8D%E6%9D%82%E8%AF%AD%E5%A2%83%E4%B8%8B%E7%9A%84%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/4.%E6%96%87%E6%A1%A3%E6%8A%BD%E5%8F%96/EOG/figure/formula_1.png)

其中 nmk 是提及节点表示，wi 是句子词表示，ai 是提及对 m1、m2 的词 i 的注意力权重，H ∈ Rw×d 是句子词表示矩阵，a ∈ Rw 是注意力权重向量 对于该对，cm1,m2 是提及对的最终上下文表示。

Mention-Sentence (MS)：仅当提及位于句子中时，才会连接提及到句子的节点。 它们的初始边缘表示被构造为提及节点和句子节点的串联，xMS = [nm;  ns]。

提及实体（ME）：如果提及与实体相关联，我们将提及节点连接到实体节点，xME = [nm;  ne]。

Sentence-Sentence (SS)：受 Quirk 和 Poon (2017) 的启发，我们连接句子节点以编码非本地信息。 与先前工作的主要区别在于我们的边缘是未标记的、无方向的并且跨越多个句子。 为了编码句子之间的距离，我们以嵌入的形式连接到句子节点表示它们的距离：xSS = [nsi;  nsj;  dsi,sj]。 我们连接图中的所有句子节点。 我们分别将 SSdirect 视为直接、有序的边（距离等于 1），将 SSindirect 视为 S 节点之间的间接、无序边（距离 > 1）。 在我们的设置中，SS 表示 SSdirect 和 SSindirect 的组合。

实体句（ES）：为了直接对实体句关联建模，我们将实体节点连接到句子节点，如果该实体至少有一个提及位于该句子中，xES = [ne;  ns]。

为了产生等维的边缘表示，我们对不同的边缘表示使用不同的线性缩减层，

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/%E5%A4%8D%E6%9D%82%E8%AF%AD%E5%A2%83%E4%B8%8B%E7%9A%84%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/4.%E6%96%87%E6%A1%A3%E6%8A%BD%E5%8F%96/EOG/figure/formula_2.png)

其中 e(1) z 是长度为 1 的边表示，Wz ∈ Rdz×d 对应于学习矩阵，z ∈ [MM, MS, ME, SS, ES]。

#### 2.4 推理层

我们利用迭代算法在图中不同节点之间生成边，以及更新现有边。 我们仅使用第 2.3.2 节中描述的边初始化图，这意味着不存在直接的实体到实体 (EE) 边。 我们只能通过表示节点之间的路径来生成 EE 边表示。这意味着实体可以通过最小长度等于 32 的边路径关联。

为此，我们调整了 Christopoulou 等人提出的两步推理机制。(2018)，对图中节点和边之间的交互进行编码，从而对 EE 关联进行建模。

第一步，我们的目标是使用中间节点 k 在两个节点 i 和 j 之间生成一条路径。 因此，我们使用修改后的双线性变换组合了两个连续边 eik 和 ekj 的表示。 此操作生成双倍长度的边表示。 我们将 i 和 j 到 k 之间的所有现有路径组合起来。  i、j 和 k 节点可以是 E、M 或 S 三种节点类型中的任何一种。与目标节点没有相邻边的中间节点将被忽略。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/%E5%A4%8D%E6%9D%82%E8%AF%AD%E5%A2%83%E4%B8%8B%E7%9A%84%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/4.%E6%96%87%E6%A1%A3%E6%8A%BD%E5%8F%96/EOG/figure/formula_3.png)

其中σ是sigmoid非线性函数，W∈Rdz×dz是学习参数矩阵，⊙指元素相乘，l是边的长度，eik对应节点i和k之间边的表示 .

在第二步中，我们使用线性插值将原始（短）边缘表示和由方程（3）产生的新（较长）边缘表示聚合如下：

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/%E5%A4%8D%E6%9D%82%E8%AF%AD%E5%A2%83%E4%B8%8B%E7%9A%84%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/4.%E6%96%87%E6%A1%A3%E6%8A%BD%E5%8F%96/EOG/figure/formula_4.png)

其中 β ∈ [0, 1] 是一个标量，用于控制较短边缘呈现的贡献。 一般来说，较短边的 β 较大，因为我们期望通过它们之间的最短路径更好地表达两个节点之间的关系（Xu 等人，2015 年；Borgwardt 和 Kriegel，2005 年）。

这两个步骤重复有限次数 N。迭代次数与边缘表示的最终长度相关。初始边长 l 等于 1，第一次迭代的结果是长度为 2 的边。第二次迭代的结果是长度为 4 的边。类似地，在 N 次迭代后，边的长度将是 2N。

#### 2.5 分类层

为了对感兴趣的概念级实体对进行分类，我们结合了一个 softmax 分类器，使用对应于概念级实体对的文档图的实体到实体边 (EE)。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/%E5%A4%8D%E6%9D%82%E8%AF%AD%E5%A2%83%E4%B8%8B%E7%9A%84%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/4.%E6%96%87%E6%A1%A3%E6%8A%BD%E5%8F%96/EOG/figure/formula_5.png)

其中 Wc ∈ Rr×dz 和 bc ∈ Rr 是分类层的学习参数，r 是关系类别的数量。

## 3 实验设置

该模型是使用 PyTorch 开发的（Paszke 等，2017）。我们结合了早期停止来确定最佳训练时期，并使用 Adam（Kingma 和 Ba，2015）作为模型优化器。

#### 3.1 数据和任务设置

我们在两个数据集上评估了提议的模型：CDR（BioCreative V）：化学疾病反应数据集由 Li 等人创建。  (2016a) 用于文档级 RE。 它由 1, 500 个 PubMed 摘要组成，它们被分成三个大小相同的集合，用于训练、开发和测试。该数据集使用化学和疾病概念之间的二元交互手动注释。对于这个数据集，我们使用了 PubMed 预训练的词嵌入（Chiu 等人，2016）。

GDA (DisGeNet)：基因-疾病关联数据集由 Wu 等人介绍。  (2019)，包含 30, 192 篇 MEDLINE 摘要，分为 29, 192 篇用于训练的文章和 1, 000 篇用于测试的文章。 该数据集使用远程监督在文档级别使用基因和疾病概念之间的二元交互进行注释。 概念之间的关联是通过将 DisGeNet (Pi~nero et al., 2016) 平台与 PubMed3 摘要对齐来生成的。 我们进一步将训练集分成 80/20 的比例作为训练集和开发集。 对于 GDA 数据集，我们使用了随机初始化的词嵌入。

#### 3.2 模型设置

我们使用不同的边（MM、ME、MS、ES、SS）和增强功能（节点类型嵌入、提及对上下文嵌入、距离嵌入）探索了提议图的多种设置。 我们将模型命名为 EoG，这是 Edge-oriented Graph 的缩写。 我们将在本节中简要描述模型设置。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/%E5%A4%8D%E6%9D%82%E8%AF%AD%E5%A2%83%E4%B8%8B%E7%9A%84%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/4.%E6%96%87%E6%A1%A3%E6%8A%BD%E5%8F%96/EOG/figure/table_1.png)

表 1：总体而言，句内和句间对在 CDR 测试集上与最新技术的性能比较。 双线以下的方法利用额外的训练数据和/或结合外部工具。

EoG 指的是我们的主模型，边为 {MM, ME, MS, ES, SS }。  EoG (Full) 设置是指具有全连接图的模型，其中图节点都相互连接，包括 E 节点。 为此，我们为 EE 边缘引入了一个额外的线性层，如等式（2）。
EoG (NoInf) 设置是指无推理模型，其中忽略迭代推理算法（第 2.4 节）。 实体节点嵌入的串联用于表示目标对。 在这种情况下，我们还为 EE 边缘使用了一个额外的 EE 线性层。 最后，EoG (Sent) 设置是指在句子而不是文档上训练的模型。 对于每个实体级别对，我们使用最大假设合并不同句子中提及级别对的预测：如果至少一个提及级别预测指示一种关系，那么我们将实体对预测为相关，类似于 Gu 等人。  (2017)。 除非另有说明，否则所有设置都包含节点类型嵌入、MM 边的上下文嵌入和 MM 和 SS 边的距离嵌入。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/%E5%A4%8D%E6%9D%82%E8%AF%AD%E5%A2%83%E4%B8%8B%E7%9A%84%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/4.%E6%96%87%E6%A1%A3%E6%8A%BD%E5%8F%96/EOG/figure/table_2.png)

表 2：GDA 开发和测试集的性能比较。

## 4 结论

表 1 描述了我们提出的模型在 CDR 测试集上的性能，与最先进的技术相比。我们直接将我们的模型与不包含外部知识的模型进行比较。 维尔加等人。  (2018) 以及 Nguyen 和 Verspoor (2018) 考虑每个文档一对，而 Gu 等人。  (2017) 为句内和句间对开发了单独的模型。 可以观察到，所提出的模型比 CDR 数据集中的最新技术高出 1.3 个百分点的整体性能。 我们还展示了利用句法依赖工具的方法。 李等人。  (2016b) 使用带有额外未标记训练数据的协同训练。我们的模型在句内和句间对上的表现要好得多，甚至与大多数具有外部知识的模型相比，除了 Li 等人。  (2016b)。

此外，我们报告了三个基线模型的性能。EoG 模型优于所有配对类型的所有基线。 特别是，对于句间对，在全连接图 (Full) 或没有推理 (NoInf) 的情况下，性能显着下降。 前者可能表明存在某些应该遵循的推理路径，以便关联驻留在不同句子中的实体。 同样重要的是要注意，句内对大大受益于文档级信息，因为 EoG 的性能比单句 (Sent) 训练的性能高 3%。最后，由于推理算法移除 (NoInf)，句内对的性能下降表明句子中存在多个实体关联（Christopoulou 等人，2018 年）。 在缺乏词上下文信息的情况下，它们的交互可能是有益的。

我们还将我们的模型应用于远程监督的 GDA 数据集。 如表 2 所示，句内对的结果与开发集和测试集的 CDR 数据集的结果一致。 这表明文档级别的信息是有帮助的。然而，句间对的性能有所不同，尤其是全连接图（完整）基线。我们将这种行为部分归因于 GDA 数据集中的少量句间对（只有 13%，而 CDR 数据集中的 30%）导致 EoG 的学习模式不足。 我们将进一步调查作为未来工作的一部分。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/%E5%A4%8D%E6%9D%82%E8%AF%AD%E5%A2%83%E4%B8%8B%E7%9A%84%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/4.%E6%96%87%E6%A1%A3%E6%8A%BD%E5%8F%96/EOG/figure/table_3.png)

表 3：EoG 在具有不同预训练词嵌入的 CDR 测试集上的性能。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/%E5%A4%8D%E6%9D%82%E8%AF%AD%E5%A2%83%E4%B8%8B%E7%9A%84%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/4.%E6%96%87%E6%A1%A3%E6%8A%BD%E5%8F%96/EOG/figure/figure_3.png)

图 3：在 CDR 开发集上使用直接 (SSdirect) 或直接和间接 (SS) 句子到句子边缘时，作为推理步骤数量的函数的性能。

## 5 分析与讨论

我们首先使用不同的预训练词嵌入来分析我们的主模型 (EoG) 的性能。表 3 显示了特定领域 (PubMed) (Chiu et al., 2016)、通用领域 (GloVe) (Pennington et al., 2014) 和随机初始化（随机）词嵌入之间的性能差异。 正如所观察到的，我们提出的模型在域内和域外预训练词嵌入方面的表现始终如一。 随机嵌入的低性能是由于数据集较小，导致嵌入质量较低。

为了进一步分析，我们选择 CDR 数据集，因为它是手动注释的。为了更好地分析我们模型的行为，我们对作为推理步骤的函数的直接和间接句子到句子边缘的影响进行了分析。图 3a、3b 和 3c 分别说明了整体、句内和句间对的两个图的性能。

第一个观察结果是，对于推理步骤 l = 8，仅使用直接边缘会使整体性能降低近 4%。这种下降主要影响句间对，其中观察到 18% 的点下降。事实上，与需要较少步骤的额外间接边 (SS) 相比，有序边 (SSdirect) 需要更长的推理才能更好地执行。 对于所有推理步骤，与 SSdirect 边缘在句间对检测上相比，SS 边缘的优越性表明，在叙述中，一些中间信息并不重要。 对于句内对 (l ≤ 16)，间接边的表现略好于直接边的观察结果与表 1 的结果一致，我们在表 1 中表明，句间信息可以作为句内对的补充证据。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/%E5%A4%8D%E6%9D%82%E8%AF%AD%E5%A2%83%E4%B8%8B%E7%9A%84%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/4.%E6%96%87%E6%A1%A3%E6%8A%BD%E5%8F%96/EOG/figure/table_4.png)

我们还对图的边和节点进行了消融分析，如表 4 所示。使用 EE 边缘只会导致对的性能不佳。去除 MM 和 ME 边缘不会显着影响性能，因为 ES 边缘可以替代它们的影响。 完全删除与 M 个节点的连接会导致句子间性能低下。 这种行为指出了一些局部依赖性在识别跨句关系中的重要性。

去除 ES 边缘会降低所有对的性能，因为 EE 边缘的编码变得更加困难 4。 我们进一步观察到没有句子到句子连接的句子间对的识别非常差。 这与模型无法识别任何没有与 S 节点连接的句间对的情况相辅相成。 在这种情况下，我们仅通过 MM 和 ME 边启用跨句子对的识别，如图 4a 所示。 在 CDR 数据集中，78% 的句间对至少有一个参数在文档中只提到过一次。 在没有 S 节点的情况下，识别这些对需要非常长的推理路径 5。 如图 4b 所示，S 节点的引入导致路径长度减半，我们希望它可以更好地表示关系。 较长的推理表示比较短的推理表示要弱得多。 这表明推理机制在识别非常复杂的关联方面的能力有限。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/%E5%A4%8D%E6%9D%82%E8%AF%AD%E5%A2%83%E4%B8%8B%E7%9A%84%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/4.%E6%96%87%E6%A1%A3%E6%8A%BD%E5%8F%96/EOG/figure/figure_4.png)

图 4：具有不同类型边的关系路径。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/%E5%A4%8D%E6%9D%82%E8%AF%AD%E5%A2%83%E4%B8%8B%E7%9A%84%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/4.%E6%96%87%E6%A1%A3%E6%8A%BD%E5%8F%96/EOG/figure/table_4.png)

表 5：CDR 开发集上边缘增强的消融分析。

然后我们研究表 5 中图边的额外增强。一般来说，句内对不受这些设置的影响。 然而，对于句间对，删除节点类型嵌入和距离嵌入会导致 F1score 下降 2% 和 5%。 这些结果表明，文档中不同元素之间的交互，以及句子和提及之间的距离，在句间对推理中起着重要作用。 删除所有这些设置的效果并不比删除其中之一差，这可能表明模型过度拟合。 作为未来工作的一部分，我们计划对此进行进一步调查。

我们根据句子级别的距离检查不同模型在句间对上的性能。 图 5 说明，对于长距离对，EoG 的性能较低，表明预测它们的难度以及对其他潜在文档级信息（EoG（完整））的可能要求。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/%E5%A4%8D%E6%9D%82%E8%AF%AD%E5%A2%83%E4%B8%8B%E7%9A%84%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/4.%E6%96%87%E6%A1%A3%E6%8A%BD%E5%8F%96/EOG/figure/figure_5.png)

图 5：作为句子距离函数的 CDR 开发集上的句间对的性能。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/%E5%A4%8D%E6%9D%82%E8%AF%AD%E5%A2%83%E4%B8%8B%E7%9A%84%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/4.%E6%96%87%E6%A1%A3%E6%8A%BD%E5%8F%96/EOG/figure/table_6.png)

表 6：来自 EoG 未能检测到的 CDR 开发集的句间对。

作为最后的分析，我们调查了一些图模型无法识别句子间相关对的情况。 为此，我们随机检查了 EoG 模型中一些常见的假阴性错误。 我们确定了三种常见的错误情况，如表 6 所示。在第一种情况下，当多个实体位于同一个句子中并用连词（例如，“and”）或逗号连接时，模型通常无法找到与所有实体的关联。 第二个错误源于缺少共指连接。 例如，膀胱输尿管炎被称为疾病。 尽管我们的模型不能直接创建这些边，但 S 节点可能通过将共同引用实体编码到句子表示中来模拟此类链接。 最后，不完整的实体链接会导致额外的模型错误。 例如，在第三个例子中，出血和颅内出血是同义词。但是，它们被分配了不同的 KB ID，因此被视为不同的实体。 该模型可以找到句内关系，但不能找到句间关系。

## 6 相关工作

传统方法侧重于句子内监督的 RE，利用 CNN 或 RNN，忽略句子中的多个实体（Zeng 等人，2014 年；Nguyen 和 Grishman，2015 年）以及结合外部句法工具（Miwa 和 Bansal，2016 年；Zhang 等人，2018 年）。 克里斯托普卢等人。(2018) 通过建模句子实体之间的长依赖关系，考虑了没有域依赖关系的句内实体交互。

其他方法处理远程监督的数据集，但也仅限于句内关系。 他们利用分段卷积神经网络 (PCNN) (Zeng et al., 2015)、注意机制 (Lin et al., 2016; Zhou et al., 2018)、实体描述符 (Jiang et al., 2016) 和图 CNNs (  Vashishth 等人，2018 年）对包含多个实体对提及的句子袋执行 MIL。 最近，曾等人。  (2017) 提出了一种提取实体之间路径的方法，使用目标实体在几个不同句子（可能在不同的文档中）中的提及作为中间连接器。 仅当这些提及属于同一实体并认为句子中存在单个提及对时，它们才允许提及-提及边缘。相反，我们不仅允许同一句子中所有提及项之间的交互，而且还考虑文档中提及项、实体和句子之间的多条边。

当前尝试处理文档级 RE 的方法主要是基于图的。  Quirk 和 Poon (2017) 引入了文档图的概念，其中节点是单词，边表示单词之间的句内和句间关系。 他们将具有不同依赖边缘的单词连接起来，并训练了一个二元逻辑回归分类器。 他们在来自 PubMed 的远程监督全文文章中评估了他们的模型，用于基因药物关联，将配对限制在连续句子的窗口内。 在这项工作之后，其他方法结合了文档级 RE 的图形模型，例如图 LSTM（Peng 等人，2017 年）、图 CNN（Song 等人，2018 年）或依赖树结构上的 RNN（Gupta 等人，2019 年）  ）。 最近，贾等人。  (2019) 使用来自文档中多个句子和段落的信息改进了 n-ary RE。 与我们的方法类似，他们选择直接对概念级别对进行分类，而不是对多个提及级别对进行分类。 尽管他们考虑子关系来为相关元组建模，但他们忽略了与话语单元中目标元组之外的其他实体的交互。

非基于图的方法利用不同的句内和句间模型并合并结果预测（Gu et al., 2016, 2017）。其他方法为每个候选实体对提取文档级表示（Zheng et al., 2018; Li et al., 2018; Wu et al., 2019），或使用句法依赖结构（Zhou et al., 2016; Peng et al., 2019） 等，2016）。 维尔加等人。  (2018) 提出了一种基于 Transformer 的模型，用于多实例学习的文档级关系提取，合并多个提及对。Nguyen 和 Verspoor (2018) 使用了带有额外字符级嵌入的 CNN。
   Singh 和 Bhatia（2019 年）还利用 Transformer 并通过直接结合上下文标记将两个目标实体连接起来。 但是，他们认为每个文档只有一个目标实体对。

## 7 总结

我们提出了一种新颖的面向边缘的图神经模型，用于使用多实例学习进行文档级关系提取。 所提出的模型构建了一个具有异构类型的节点和边的文档级图，同时使用图边上的迭代算法对句内和句间对进行建模。

据我们所知，这是将面向边缘的模型用于文档级 RE 的第一种方法。对句内和句间对的分析表明，所提出的、部分连接的文档图结构可以有效地编码文档元素之间的依赖关系。此外，我们推断文档级信息有助于识别句内对，从而获得更高的精度和 F1score。

作为未来的工作，我们计划改进推理机制，并可能在文档图结构中加入额外的信息。 我们希望这项研究能够激励社区进一步研究边缘模型在 RE 和其他相关任务上的使用。





















