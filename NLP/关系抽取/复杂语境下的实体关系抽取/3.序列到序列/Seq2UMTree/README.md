# 最小化联合实体和关系提取中 Seq2Seq 模型的暴露偏差

# Minimize Exposure Bias of Seq2Seq Models in Joint Entity and Relation
Extraction

## 摘要

联合实体和关系提取旨在直接从纯文本中提取关系三元组。先前的工作利用序列到序列 (Seq2Seq) 模型来生成三元组序列。 然而，Seq2Seq 对无序三元组强制执行不必要的顺序，并且涉及与错误累积相关的大解码长度。 这些方法引入了曝光偏差，这可能会导致模型过度拟合频繁的标签组合，从而限制了泛化能力。我们提出了一种新的 Sequence-to-UnorderedMulti-Tree (Seq2UMTree) 模型，通过将三元组中的解码长度限制为三个并去除三元组之间的顺序来最小化曝光偏差的影响。 我们在两个数据集 DuIE 和 NYT 上评估我们的模型，并系统地研究曝光偏差如何改变 Seq2Seq 模型的性能。 实验表明，最先进的 Seq2Seq 模型对两个数据集都过拟合，而 Seq2UMTree 显示出明显更好的泛化。 我们的代码可在 https://github.com/WindChimeRan/OpenJERE 获得。

## 1 介绍

关系提取旨在从纯文本中提取实体-关系三元组 (h, r, t)。 例如，在三元组（Obama，毕业于哥伦比亚大学）中，Obama 和 Columbia University 是出现在文本中的头尾实体，而 graduate from 是这两个实体之间的关系。

对于有监督的关系抽取，早期的研究集中在管道方法上，它使用实体抽取器来抽取实体，然后对实体对的关系进行分类。 这些方法忽略了这两个子任务之间的内在交互，并通过任务传播分类错误。联合实体和关系提取（JERE）考虑子任务交互（Roth 和 Yih，2004 年；Ji 和 Grishman，2005 年； 姬等人，2005； 于和林，2010 年； 里德尔等人，2010 年；  Sil 和 Yates，2013 年； 李等人，2014 年； 李和姬，2014；  Durrett 和 Klein，2014 年；  Miwa 和 Sasaki，2014 年； 卢和罗斯，2015； 杨和米切尔，2016 年；  Kirschnick 等人，2016 年； 米瓦和班萨尔，2016 年； 古普塔等人，2016 年；  Katiyar 和 Cardie，2017），但他们主要利用基于特征的系统或多任务神经网络，无法捕获三元组间的依赖关系。NovelTagging (Zheng et al., 2017) 将这两个子任务集成到一个序列标记过程中，该过程为每个标记分配一个实体关系标签； 当一个token属于多个关系时，预测结果是不完整的。  Sequence-toSequence (Seq2Seq) 模型（Cho 等人，2014 年）能够多次提取一个实体而不是序列标记，因此可以将多个关系分配给一个实体，这自然地解决了这个问题（Zeng 等人，  2018, 2019a,b；Nayak 和 Ng，2019)。具体来说，所有现有的 Seq2Seq 模型都预先定义了目标三元组的顺序，例如 三元组字母顺序，然后根据顺序自回归解码三元组序列，这意味着当前的三元组预测依赖于之前的输出。 例如，在图 1 中，三元组列表被扁平化为[Obama]-[graduate from]-[Columbia University]-[Obama]-[graduate from]-[Harvard Law School]...

然而，Seq2Seq 模型的自回归解码引入了暴露偏差问题，这可能会严重降低性能。 曝光偏差是指解码过程的训练和测试阶段之间的差异（Ranzato 等，2015）。 在训练阶段，当前的三元组预测依赖于之前三元组的黄金标准标签，而在测试阶段，当前的三元组预测依赖于之前三元组的模型预测，这可以与黄金标准不同 标签。 因此，在测试阶段，偏斜的预测会进一步偏离后续三元组的预测； 如果解码长度很大，与金标准标签的差异将进一步累积。 这种累积的差异可能会降低性能，尤其是在预测更长的序列时，即多三元组预测.

![]()

图 1：针对不同三元组顺序的 Seq2Seq 和 Seq2UMTree 的训练和测试。

此外，由于 Seq2Seq 模型顺序预测三元组，因此它对无序标签强制执行了不必要的顺序，而其他三元组顺序也是正确的。 因此，分配的顺序使模型容易记住和过度拟合训练集中的频繁标签组合，并且对看不见的顺序的泛化能力很差。 过度拟合也是曝光偏差的副作用（Tsai 和 Lee，2019），这可能导致 Seq2Seq 预测中缺少三元组。 例如，在图1中，在训练阶段，Seq2Seq模型以这样的顺序学习triplet1-triplet2-triplet3，而triplet2-triplet1-triplet3的顺序也是正确的。 在测试阶段，Seq2Seq 模型首先根据指定的顺序预测triplet2，但是因为triplet2-triplet3 是模型的频繁顺序，所以它忽略了triplet1 并直接以triplet3 结尾（即triplet2-triplet3）。 当对模型强制执行命令时，模型会继续进行更多的学习约束。

为了在保持 Seq2Seq 简单性的同时减轻曝光偏差问题，我们将一维三元组序列重新转换为二维无序多树 (UMTree)，并提出了一种新的模型 Seq2UMTree。
   Seq2UMTree 模型基于 Encoder-Decoder 框架，由传统编码器和 UMTree 解码器组成。  UMTree 解码器使用具有无序多标签分类的复制机制作为输出层，对实体和关系进行联合和结构化建模。 这种多标签分类模型确保同一层中的节点是无序的，并丢弃预定义的三元组顺序，这样预测偏差就不会聚合并影响其他三元组。 与标准 Seq2Tree（Dong 和 Lapata，2016；Liu 等，2019）不同，解码长度限制为三个（一个三元组），这是 JERE 任务的最短可行长度。 通过这种方式，在三重态 F1 指标下，曝光偏差被最小化。

总之，我们的贡献如下：

- 我们指出了 Seq2Seq 模型的预定义三元组顺序的冗余，并提出了一种新颖的 Seq2UMTree 模型，通过将有序三元组序列重新转换为无序多树格式来最小化曝光偏差。 
-  我们系统地分析了暴露偏差如何降低标准 Seq2Seq 模型性能分数的可靠性。

![]()

图 2：模型概述。 三元组内的解码顺序是 r, t, h。 关系是从预定义的关系字典中预测出来的，实体是从句子中复制出来的。

## 2 方法

Seq2UMTree 模型由一个传统的 Seq2Seq 编码器和一个 UMTree 解码器组成。  UMTree 解码器与标准解码器的不同之处在于它生成无序多标签输出并使用 UMTree 解码策略。 该模型的概述如图所示2.我们在以下小节中说明模型细节。

#### 2.1 模型

形式上，输入句子 x = [x0, x1, .  .  .  , xn] 首先通过词嵌入和双向循环神经网络 (Bi-RNN) (Schuster and Paliwal, 1997) 和长短期记忆 (LSTM) (Hochreiter and Schmidhuber, 1997) 转换为一系列上下文感知表示 编码器：





