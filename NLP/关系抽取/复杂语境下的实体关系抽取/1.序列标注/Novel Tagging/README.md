# 基于新标注方案的实体与关系联合抽取

# ( Joint Extraction of Entities and Relations Based on Novel Tagging Scheme )

## 摘要

实体和关系的联合抽取是信息抽取中的一项重要任务。为了解决这个问题，我们首先提出了一种新的标记方案，可以将联合提取任务转换为标记问题。然后，基于我们的标记方案，我们研究了不同的端到端模型来直接提取实体及其关系，而无需分别识别实体和关系。 我们对远程监督方法产生的公共数据集进行了实验，实验结果表明基于标记的方法优于大多数现有的流水线和联合学习方法。 更重要的是，本文提出的端到端模型在公共数据集上取得了最好的结果。

## 1 前言

实体和关系的联合提取是从非结构化文本中同时检测实体提及并识别它们的语义关系，如图1所示。 与从给定句子中提取关系词的开放信息提取 (Open IE) (Banko et al., 2007) 不同，在该任务中，关系词是从给定句子中可能不会出现的预定义关系集中提取的。 是知识提取和知识库自动构建中的一个重要问题。

传统方法以流水线方式处理此任务，即首先提取实体（Nadeau 和 Sekine，2007），然后识别它们的关系（Rink，2010）。 这种分离的框架使任务易于处理，每个组件都可以更加灵活。 但是它忽略了这两个子任务之间的相关性，每个子任务都是一个独立的模型。 实体识别的结果可能会影响关系分类的性能并导致错误传递（Li and Ji, 2014）。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/%E5%A4%8D%E6%9D%82%E8%AF%AD%E5%A2%83%E4%B8%8B%E7%9A%84%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/1.%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/Novel%20Tagging/Figure/figure_1.png)

图 1：任务的标准例句。  “国家主席”是预定义关系集中的关系。

与流水线方法不同，联合学习框架是使用单个模型将实体和关系一起提取。 它可以有效地整合实体和关系的信息，并已被证明在该任务中取得了更好的效果。 然而，大多数现有的联合方法是基于特征的结构化系统（Li 和 Ji，2014；Miwa 和 Sasaki，2014；Yu 和 Lam，2010；Ren 等，2017）。 他们需要复杂的特征工程并严重依赖其他 NLP 工具包，这也可能导致错误传播。 为了减少特征提取中的手动工作，最近（Miwa 和 Bansal，2016）提出了一种基于神经网络的端到端实体和关系提取方法。 尽管联合模型可以在单个模型中表示具有共享参数的实体和关系，但它们也分别提取实体和关系并产生冗余信息。例如，图 1 中的句子包含三个实体：“美国”、“特朗普”和“苹果公司”。 但只有“美国”和“特朗普”有固定的“国家总统”关系。实体“Apple Inc”与本句中的其他实体没有明显关系。 因此，从这句话中提取的 arXiv:1706.05075v1 [cs.CL] 7 Jun 2017 结果是 {United States<sub>e1</sub>, Country-President<sub>r</sub>, Trump<sub>e2</sub>}，这里称为三元组。

在本文中，我们专注于提取由两个实体和这两个实体之间的一种关系组成的三元组。 因此，我们可以直接对三元组进行建模，而不是分别提取实体和关系。 基于这些动机，我们提出了一种带有端到端模型的标记方案来解决这个问题。 我们设计了一种新颖的标签，其中包含实体的信息及其持有的关系。 基于这种标注方案，实体和关系的联合抽取可以转化为标注问题。 这样，我们也可以轻松地使用神经网络对任务进行建模，而无需进行复杂的特征工程。

最近，基于 LSTM（Hochreiter 和 Schmidhuber，1997）的端到端模型已成功应用于各种标记任务：命名实体识别（Lample 等，2016）、CCG 超级标记（Vaswani 等，2016）、  Chunking (Zhai et al., 2017) et al.LSTM 能够学习长期依赖，这有利于序列建模任务。

因此，基于我们的标记方案，我们研究了不同类型的基于 LSTM 的端到端模型来联合提取实体和关系。 我们还通过添加偏置损失来修改解码方法，使其更适合我们的特殊标签。我们提出的方法是一种监督学习算法。

然而，实际上，手动标记具有大量实体和关系的训练集的过程过于昂贵且容易出错。 因此，我们对通过远程监督方法（Ren et al., 2017）产生的公共数据集进行实验以验证我们的方法。 实验结果表明我们的标记方案在这项任务中是有效的。 此外，我们的端到端模型可以在公共数据集上取得最好的结果。

本文的主要贡献是：（1）提出了一种新的标注方案来联合提取实体和关系，可以很容易地将提取问题转化为标注任务。(2) 基于我们的标记方案，我们研究了不同种类的端到端模型来解决问题。 基于标记的方法优于大多数现有的流水线和联合学习方法。  (3) 此外，我们还开发了一个具有偏置损失函数的端到端模型，以适应新标签，它可以增强相关实体之间的关联。

## 2 相关工作

实体与关系抽取是构建知识库的重要步骤，可为许多NLP任务带来益处。两个主要框架已被广泛用于解决提取实体及其关系的问题。一个是**流水线方法**，另一个是**联合学习方法**。

**流水线方法将这个任务视为两个分离的任务**，即命名实体识别（NER）（Nadeau和Sekine，2007）和关系分类（RC）（Rink，2010）。经典的NER模型是线性统计模型，如隐马尔可夫模型（HMM）和条件随机场（CRF）（Passos等，2014; Luo等，2015）。最近，几个神经网络体系结构（Chiu和Nichols，2015; Huang等，2015; Lample等，2016）已经成功应用于NER，这被认为是一个连续的分词标记任务。现有的关系分类方法也可以分为基于手工特征的方法（Rink，2010; Kambhatla，2004）和基于神经网络的方法（Xu，2015a; Zheng et al., 2016; Zeng，2014; Xu，2015b; dos Santos ，2015）。

**联合模型使用单一模型提取实体和关系**。大多数联合方法是基于特征的结构化系统（Ren等，2017; Yang和Cardie，2013; Singh等，2013; Miwa和Sasaki，2014; Li和Ji，2014）。最近，（Miwa和Bansal，2016）使用基于LSTM的模型来提取实体和关系，这可以减少人工工作。

与上述方法不同的是，**本文提出的方法是基于一种特殊的标记方式，使得我们可以很容易地使用端到端模型来提取结果而不需要NER（命名实体识别）和RC（关系分类）**。端到端的方法是将输入句子映射成有意义的向量，然后返回产生一个序列。它广泛应用于机器翻译（Kalchbrenner和Blunsom，2013; Sutskever等，2014）和序列标注任务（Lample等，2016; Vaswani等，2016）。大多数方法使用双向LSTM来对输入句子进行编码，但是解码方法总是不同的。例如，（Lample等，2016）使用CRF层来解码标注序列，而（Vaswani等，2016; Katiyar和Cardie，2016）应用LSTM层来产生标注序列。

## 3 方法

我们提出了一种新的标注方案和一个具有偏置目标函数的端到端模型来共同提取实体及其关系。在本节中，我们首先介绍如何将提取问题转换为基于本文标注方法的标注问题。然后我们将详细说明用来提取结果的模型。

#### 3.1、The Tagging Scheme（标注方案）

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/%E5%A4%8D%E6%9D%82%E8%AF%AD%E5%A2%83%E4%B8%8B%E7%9A%84%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/1.%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/Novel%20Tagging/Figure/figure_2.png)

图二： “CP”是“Country-President”的简称，“CF”是“Company-Founder”的简称，是一个基于我们标注方案的例句的标准黄金标注方案。

图2是标注结果的示例。每个单词都被分配一个标签，用于提取结果。**标签“O”代表“Other”标签**，这意味着相应的单词与提取结果无关。除了“O”之外，其他标签由**三部分组成**：**实体中的单词位置**、**关系类型**和**关系角色**。我们**使用“BIES”（Begin, Inside, End, Single）符号来表示单词在实体中的位置信息**。**关系类型信息是从一组预定义的关系中获得的**，**关系角色信息由数字“1”和“2”表示**。提取的结果由三元组表示：（Entity1，RelationType，Entity2）。“1”表示该词属于三元组中的第一个实体，而“2”则属于该关系类型后面的第二个实体。因此，标签总数为Nt = 2 * 4 * | R | + 1，其中| R |是预定义的关系集的大小。

图2是一个说明我们的标注方法的例子。输入句子包含两个三元组：{United States, Country-President, Trump}和{Apple Inc, Company-Founder, Steven Paul Jobs}，其中“Country-President”和“Company-Founder”是预定义的关系类型。United”,“States”,“Trump”,“Apple”,“Inc” ,“Steven”, “Paul”和“Jobs”等词都与最终提取的结果有关。因此，他们根据我们的特殊标签进行标注。例如“United”这个词是“United States”实体的第一个词，与“Country-President”关系有关，所以它的标签是“B-CP-1”。对应于“United States”的另一个实体“Trump”被标记为“S-CP-2”。此外，与最终结果无关的其他字词标记为“O”。

#### 3.2、From Tag Sequence To Extracted Results（从标记序列到提取结果）

根据图2中的标注序列，我们知道“Trump”和“United States”具有相同的关系类型“Country-President”，“Apple Inc”和“Steven Paul Jobs”具有相同的关系类型“Company-Founder”。我们**将具有相同关系类型的实体合并为一个三元组来获得最终结果**。因此，“Trump”和“United States”可以合并为关系类型为“Country-President”的三联体。因为，“Trump”的关系角色是“2”，“United States”是“1”，最终的结果是{United States, CountryPresident, Trump}。这同样适用于{Apple Inc, Company-Founder, Steven Paul Jobs}。

此外，**如果一个句子包含两个或更多具有相同关系类型的三元组，我们将每两个元素按照最接近的原则组合成一个三元组**。例如，如果图2中的关系类型“Country-President”是“Country-President”，则在给定句子中将有四个具有相同关系类型的实体。 “United States”最接近实体“Trump”，而“Apple Inc”最接近“Jobs”，因此结果将是{United States, Company-Founder, Trump}和{Apple Inc, Company-Founder, Steven Paul Jobs}。

在本文中，我们只考虑**一个实体属于一个三元组**的情况，并且在将来的工作中考虑重叠关系的识别。

#### 3.3、The End-to-end Model（端到端模型）

近年来，基于神经网络的端到端模型在序列标注任务中得到了广泛的应用。在本文中，我们采用了一个端到端的模型来生成标注序列，如图3所示。它包含双向长短期记忆（Bi-LSTM）层来对输入句子和具有偏置损失的基于LSTM的解码层进行编码。**偏置损失可以增强实体标签的相关性**。

![](https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/%E5%A4%8D%E6%9D%82%E8%AF%AD%E5%A2%83%E4%B8%8B%E7%9A%84%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/1.%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/Novel%20Tagging/Figure/figure_3.png)

图三：我们的模型图。 （a）**端到端模型的体系结构**，（b）**Bi-LSTM编码层中的LSTM记忆块**，（c）**LSTMd解码层中的LSTM记忆块**。

**（1）Bi-LSTM编码层**
**在序列标注问题中，Bi-LSTM编码层已被证明有效捕获每个单词的语义信息**。它包含前向Lstm层，后向Lstm层和连接层。**词嵌入层将one-hot表示的单词转换为嵌入向量**。因此，一个单词序列可以表示为W = {w1，… wt，wt+1 … wn}，其中wt∈Rd是对应于句中第t个单词的d维词向量，n是给定句子的长度。在词嵌入层之后，有两个平行的LSTM层：前向LSTM层和后向LSTM层。 LSTM体系结构由一组递归连接的子网（称为记忆块）组成。每个时间步是一个LSTM记忆块。 Bi-LSTM编码层中的LSTM记忆块用于根据前一个隐藏向量ht-1、前一个单元向量ct-1和当前输入词表示wt计算当前隐藏向量ht。其结构图如图3（b）所示，具体操作定义如下：

<img src="https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/%E5%A4%8D%E6%9D%82%E8%AF%AD%E5%A2%83%E4%B8%8B%E7%9A%84%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/1.%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/Novel%20Tagging/Figure/formula_1.png" style="zoom:50%;" />

其中i，f和o分别是输入门、忘记门和输出门，b是偏置项，c是记忆元，W(.)是参数。对于每个词wt，前向LSTM层将通过考虑从词w1到wt的上下文信息（其被标记为ht(→)）来编码wt。类似地，后向LSTM层将基于从wn到wt的上下文信息来编码wt，其被标记为ht(←)。最后，我们连接和来表示字t的编码信息，表示为ht=[ht(→),ht(←)]。

**（2）LSTM解码器层**
我们也**采用LSTM结构来生成标注序列**。当检测到单词wt的标注时，解码层的输入为：从Bi-LSTM编码层获得的ht，以前的预测标签表示Tt-1，以前的单元值：ct-1，以及解码层中的前一个隐藏向量ht-1。图3（c）显示了LSTMd记忆块的结构图，具体操作定义如下：

<img src="https://gitee.com/Xiaoyingzi09/note-book/raw/master/NLP/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/%E5%A4%8D%E6%9D%82%E8%AF%AD%E5%A2%83%E4%B8%8B%E7%9A%84%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/1.%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/Novel%20Tagging/Figure/formula_2.png" style="zoom:50%;" />









